<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[修改hdfs和yarn的pid目录]]></title>
    <url>%2F2019%2F03%2F02%2F%E4%BF%AE%E6%94%B9hdfs%E5%92%8Cyarn%E7%9A%84pid%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Linux操作系统中会默认定期清理/tmp目录中的文件，而hdfs和yarn的pid文件默认存放在/tmp目录下，为避免pid文件被误删，修改默认的文件目录 修改hdfs集群的pid文件位置 配置HADOOP_PID_DIR 1234cd ~/app/hadoop-2.6.0-cdh5.7.0/vi etc/hadoop/hadoop-env.sh # 修改HADOOP_PID_DIR的值export HADOOP_PID_DIR=~/data/tmp 重启hadoop集群 12345[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ ll ~/data/tmptotal 12-rw-rw-r-- 1 hadoop hadoop 6 Mar 2 12:39 hadoop-hadoop-datanode.pid-rw-rw-r-- 1 hadoop hadoop 6 Mar 2 12:38 hadoop-hadoop-namenode.pid-rw-rw-r-- 1 hadoop hadoop 6 Mar 2 12:39 hadoop-hadoop-secondarynamenode.pid 修改yarn的pid目录 配置YARN_PID_DIR 123[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ vim etc/hadoop/yarn-env.sh # 新增export YARN_PID_DIR=/home/hadoop/data/tmp 重启yarn 12stop-yarn.shstart-yarn.sh 1234567[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ ll ~/data/tmp总用量 20-rw-rw-r--. 1 hadoop hadoop 6 4月 30 21:35 hadoop-hadoop-datanode.pid-rw-rw-r--. 1 hadoop hadoop 6 4月 30 21:35 hadoop-hadoop-namenode.pid-rw-rw-r--. 1 hadoop hadoop 6 4月 30 21:35 hadoop-hadoop-secondarynamenode.pid-rw-rw-r--. 1 hadoop hadoop 6 4月 30 21:40 yarn-hadoop-nodemanager.pid-rw-rw-r--. 1 hadoop hadoop 6 4月 30 21:40 yarn-hadoop-resourcemanager.pid]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>大数据</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yarn伪分布式部署]]></title>
    <url>%2F2019%2F03%2F02%2FYarn%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[目标：部署yarn伪分布式，并使MapReduce作业跑在yarn之上 MapReduce 是用来做计算的 是jar包提交到yarn上 本身不需要部署Yarn：资源和作业调度，需要单独部署 配置yarn和MapReduce 配置mapred-site.xml指定MapReduce调度器 1234567891011su - hadoopcd app/hadoop-2.6.0-cdh5.7.0cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xmlvi etc/hadoop/mapred-site.xml# 添加&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置yarn-site.xml 12345678vi etc/hadoop/yarn-site.xml&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动yarn 1234[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ sbin/start-yarn.sh starting yarn daemonsstarting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop614.outhadoop614: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop614.out 我们发现yarn启动了两个进程：resourcemanager #是一个资源管理者nodemanager # 是一个节点管理者同时yarn提供了一个资源调度的UI页面：http://hadoop614:8088/cluster 测试MapReduce 计算圆周率1hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 5 10 可以在yarn的UI页面查看作业详情。 计算单词出现的次数（词频统计）12345678910111213141516171819202122232425262728293031323334353637383940414243444546## 创建a.log[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ vi a.logruozejepsonwww.ruozedata.comdashuadaifanren1abca b c ruoze jepon## 创建b.log[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ vi b.loga b d e f ruoze1 1 3 5## 创建目录[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -mkdir -p /wordcount/input# 上传文件至/wordcount/input[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -put a.log /wordcount/input[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -put b.log /wordcount/input[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -ls /wordcount/input/-rw-r--r-- 1 hadoop supergroup 75 2019-04-30 21:16 /wordcount/input/a.log-rw-r--r-- 1 hadoop supergroup 24 2019-04-30 21:17 /wordcount/input/b.log[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /wordcount/input /wordcount/output[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -ls /wordcount/output-rw-r--r-- 1 hadoop supergroup 0 2019-04-30 21:19 /wordcount/output/_SUCCESS-rw-r--r-- 1 hadoop supergroup 107 2019-04-30 21:19 /wordcount/output/part-r-00000[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -text /wordcount/output/part*1 33 15 1a 3adai 1b 3c 2d 1dashu 1e 1f 1fanren 1jepon 1jepson 1ruoze 3www.ruozedata.com 1]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>大数据</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解读jps命令和hadoop pid文件]]></title>
    <url>%2F2019%2F03%2F02%2F%E8%A7%A3%E8%AF%BBjps%E5%91%BD%E4%BB%A4%E5%92%8Chadoop-pid%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[jps命令的真相首先我们先看jps命令的位置12$ which jps/usr/java/jdk1.8.0_45/bin/jps 我们发现jps命令实在java目录下，说明jps是java JDK中的一个命令；jps查看的时当前用户运行的java进程 对应进程的表示文件位置哪个用户起的java进程，其对应的进程表示文件在/tmp/hsperfdata_${USER}目录 1234567891011121314151617[hadoop@hadoop614 ~]$ cd /tmp/hsperfdata_$&#123;USER&#125;[hadoop@hadoop614 hsperfdata_hadoop]$ ll总用量 192-rw-------. 1 hadoop hadoop 32768 4月 30 20:28 27874-rw-------. 1 hadoop hadoop 32768 4月 30 20:29 27994-rw-------. 1 hadoop hadoop 32768 4月 30 20:29 28235-rw-------. 1 hadoop hadoop 32768 4月 30 20:29 28451-rw-------. 1 hadoop hadoop 32768 4月 30 20:29 28576-rw-------. 1 hadoop hadoop 32768 4月 30 20:28 28632[hadoop@hadoop614 hsperfdata_hadoop]$ jps28576 NodeManager27874 NameNode29154 Jps28451 ResourceManager28632 RunJar27994 DataNode28235 SecondaryNameNode 那么root作为超级用户，权限最高的用户，理论上root应该可以查看所有用户下的进程，以root执行 jps 命令的结果是什么呢 12345678[root@hadoop614 ~]# jps29248 Jps28576 -- process information unavailable27874 -- process information unavailable28451 -- process information unavailable28632 -- process information unavailable27994 -- process information unavailable28235 -- process information unavailable 我们可以看到使用root用户执行结果的进程数和pid与hadoop用户执行结果一致，但是进程名称却显示为process information unavailable,是不用状态；因此，当我门需要以root用户来判断进程是否正常时，需要以ps -ef返回结果为准，以hadoop进程为例：123[root@hadoop614 ~]# ps -ef | grep namenode &amp;&amp; grep datanamehadoop 27874 1 4 20:28 ? 00:00:24 /usr/java/jdk1.8.0_45/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs -Dhadoop.log.file=hadoop-hadoop-namenode-hadoop614.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNodehadoop 28235 1 2 20:28 ? 00:00:16 /usr/java/jdk1.8.0_45/bin/java -Dproc_secondarynamenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs -Dhadoop.log.file=hadoop-hadoop-secondarynamenode-hadoop614.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode 当我们以root权限kill hadoop进程后可能发生的现象1234567891011[root@hadoop614 ~]# kill -9 28576[root@hadoop614 ~]# ps -ef | grep 28576root 29407 29228 0 20:41 pts/1 00:00:00 grep 28576[root@hadoop614 ~]# jps29408 Jps28576 -- process information unavailable27874 -- process information unavailable28451 -- process information unavailable28632 -- process information unavailable27994 -- process information unavailable28235 -- process information unavailable 我们发现使用root用户强制kill hadoop进程后，进程已经不存在，但是jps命令依然可以查到那么看下jps命令pid存放路径12345678[root@hadoop614 ~]# ll /tmp/hsperfdata_hadoop总用量 192-rw-------. 1 hadoop hadoop 32768 4月 30 20:42 27874-rw-------. 1 hadoop hadoop 32768 4月 30 20:42 27994-rw-------. 1 hadoop hadoop 32768 4月 30 20:42 28235-rw-------. 1 hadoop hadoop 32768 4月 30 20:42 28451-rw-------. 1 hadoop hadoop 32768 4月 30 20:40 28576-rw-------. 1 hadoop hadoop 32768 4月 30 20:42 28632 发现28576进程的文件依然存在，尝试删除此文件；123456789101112131415[root@hadoop614 ~]# rm -f /tmp/hsperfdata_hadoop/28576 [root@hadoop614 ~]# ll /tmp/hsperfdata_hadoop总用量 160-rw-------. 1 hadoop hadoop 32768 4月 30 20:44 27874-rw-------. 1 hadoop hadoop 32768 4月 30 20:44 27994-rw-------. 1 hadoop hadoop 32768 4月 30 20:44 28235-rw-------. 1 hadoop hadoop 32768 4月 30 20:44 28451-rw-------. 1 hadoop hadoop 32768 4月 30 20:44 28632[root@hadoop614 ~]# jps27874 -- process information unavailable28451 -- process information unavailable29464 Jps28632 -- process information unavailable27994 -- process information unavailable28235 -- process information unavailable 发现jps中进程已经不存在，说明使用root用户kill掉jps进程时，还需要删除相应目录下的pid文件，否则jps查询时次pid仍然存在，而hadoop启动和停止时会校验这个文件，核能到时hadoop集群启动失败]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>大数据</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以hostname启动hadoop伪分布式]]></title>
    <url>%2F2019%2F03%2F02%2F%E4%BB%A5hostname%E5%90%AF%E5%8A%A8hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[搭建hdfs伪分部署 ： 点击跳转 通过《搭建hdfs伪分部署》，在启动阶段发现：namenode、namenode、secondarynamenode是以localhost和0.0.0.0启动的；本片文章要实现的是以hostname来启动进程;因为在日常生产中均以hostname来部署启动，具体方便在哪里，后面体会吧、su - hadoop 配置core-site.xml文件，修改namenode进程 1234567vi $HADOOP_HOME/etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop614:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置slaves文件，修改datanode进程 12vi $HADOOP_HOME/etc/hadoop/slaveshadoop614 配置 hdfs-site.xml 文件 修改 secondarynamenode 进程 123456789101112131415vi $HADOOP_HOME/etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop614:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop614:50091&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动hdfs 123456789start-dfs.sh## 打印内容中可看到，三个进程均是以hadoop614启动，hadoop614是我的hostname19/03/02 11:19:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [hadoop614]hadoop614: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop614.outhadoop614: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop614.outStarting secondary namenodes [hadoop614]hadoop614: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop614.out19/03/02 11:19:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>大数据</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建hdfs伪分部署]]></title>
    <url>%2F2019%2F03%2F02%2F%E6%90%AD%E5%BB%BAhdfs%E4%BC%AA%E5%88%86%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[软件版本 软件 Hadoop JDK 版本 2.6.0-cdh5.7.0 jdk1.8.0_45 部署JDK飞机票：点击跳转配置ssh无密码访问localhost：点击跳转 1.创建hadoop用户，管理所有hadoop生态软件1234# 创建用户user add# 切换到hadoop用户su - hadoop 2.创建目录1234mkdir app # hadoop组件安装目录mkdir software # hadoop组件安装包下载路径mkdir data # hadoop组件数据存放路径mkdir source # hadoop组件源码路径 3.下载hadoop-2.6.0-cdh5.7.012cd softwarewget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz 4.解压hadoop-2.6.0-cdh5.7.01tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ../app/ 5.配置etc/hadoop/hadoop-env.sh脚本12345cd ../app/vim etc/hadoop/hadoop-env.sh# The java implementation to use.export JAVA_HOME=/usr/java/jdk1.8.0_45export HADOOP_PREFIC=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 6.编辑配置文件1234567vim etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 1234567vim etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7.配置环境变量12345echo &apos;&apos;&apos;export HADOOP_PREFIX=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0export HADOOP_HOME=$&#123;HADOOP_PREFIX&#125;export PATH=$&#123;HADOOP_PREFIX&#125;/bin:PATH=$&#123;HADOOP_PREFIX&#125;/sbin:$PATH&apos;&apos;&apos; &gt;&gt; ~/.bash_profile 8.启动hdfs123456789101112131415cd ~/app/hadoop-2.6.0-cdh5.7.0bin/hdfs namenode -format# 打印类似以下内容19/03/02 10:22:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [localhost]localhost: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop614.outlocalhost: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop614.outStarting secondary namenodes [0.0.0.0]The authenticity of host &apos;0.0.0.0 (0.0.0.0)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:FHC9m3uE6QV24WscKKQIPROVQv0RpFOrpOYl8xjJNpE.ECDSA key fingerprint is MD5:ec:90:80:a1:85:3f:3b:f2:d7:d9:0c:79:48:ce:56:f1.Are you sure you want to continue connecting (yes/no)? yes0.0.0.0: Warning: Permanently added &apos;0.0.0.0&apos; (ECDSA) to the list of known hosts.0.0.0.0: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop614.out19/03/02 10:22:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 9.查看是否启动成功123456jps# 打印10978 NameNode11368 Jps11259 SecondaryNameNode11101 DataNode 10.使用hdfs命令帮助123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990hdfs# 打印 Usage: hdfs [--config confdir] COMMAND where COMMAND is one of: dfs run a filesystem command on the file systems supported in Hadoop. namenode -format format the DFS filesystem secondarynamenode run the DFS secondary namenode namenode run the DFS namenode journalnode run the DFS journalnode zkfc run the ZK Failover Controller daemon datanode run a DFS datanode dfsadmin run a DFS admin client haadmin run a DFS HA admin client fsck run a DFS filesystem checking utility balancer run a cluster balancing utility jmxget get JMX exported values from NameNode or DataNode. mover run a utility to move block replicas across storage types oiv apply the offline fsimage viewer to an fsimage oiv_legacy apply the offline fsimage viewer to an legacy fsimage oev apply the offline edits viewer to an edits file fetchdt fetch a delegation token from the NameNode getconf get config values from configuration groups get the groups which users belong to snapshotDiff diff two snapshots of a directory or diff the current directory contents with a snapshot lsSnapshottableDir list all snapshottable dirs owned by the current user Use -help to see options portmap run a portmap service nfs3 run an NFS version 3 gateway cacheadmin configure the HDFS cache crypto configure HDFS encryption zones storagepolicies list/get/set block storage policies version print the version Most commands print help when invoked w/o parameters. [hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs Usage: hadoop fs [generic options] [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] [-cat [-ignoreCrc] &lt;src&gt; ...] [-checksum &lt;src&gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-count [-q] [-h] [-v] &lt;path&gt; ...] [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;] [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] [-du [-s] [-h] &lt;path&gt; ...] [-expunge] [-find &lt;path&gt; ... &lt;expression&gt; ...] [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-getfacl [-R] &lt;path&gt;] [-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] [-mkdir [-p] &lt;path&gt; ...] [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] [-mv &lt;src&gt; ... &lt;dst&gt;] [-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] [-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;] [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] [-stat [format] &lt;path&gt; ...] [-tail [-f] &lt;file&gt;] [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-usage [cmd ...]] Generic options supported are -conf &lt;configuration file&gt; specify an application configuration file -D &lt;property=value&gt; use value for given property -fs &lt;local|namenode:port&gt; specify a namenode -jt &lt;local|resourcemanager:port&gt; specify a ResourceManager -files &lt;comma separated list of files&gt; specify comma separated files to be copied to the map reduce cluster -libjars &lt;comma separated list of jars&gt; specify comma separated jar files to include in the classpath. -archives &lt;comma separated list of archives&gt; specify comma separated archives to be unarchived on the compute machines. The general command line syntax is bin/hadoop command [genericOptions] [commandOptions]]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>大数据</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门篇]]></title>
    <url>%2F2019%2F03%2F02%2FHadoop%E5%85%A5%E9%97%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[问：什么是Hadoop？答： 从广义上来讲，是以Apache Hadoop软件为中心的生态圈，包括但不限于：hive、zookeeper、spark、hbase等 从狭义上讲，就是 Apache Hadoop软件 Hadoop 是Apache基金会下一个开源的分布式计算平台，它以分布式文件系统HDFS和MapReduce算法为核心，为用户提供了系统底层细节透明的分布式基础架构。 hadoop的优点优点：Hadoop是一个开源框架，可编写和运行分布式应用来处理大规模数据，分布式计算是一个不断变化且宽泛的领域，优点如下： 1. 易用性。Hadoop运行在由一般商用机器构成的大型集群上。 2. 可靠性。Hadoop致力于一般商用机器上，其架构假设硬件会频繁出现失效，它可以从容处理大多数此类故障。 3. 可扩展。Hadoop通过增加集群节点，可以线性地拓展以处理更大数据集。 4. 简单。Hadoop允许用户快速的编写出高效地并行代码。 比较Hadoop和SQL数据库从总体上看，现在大多数数据应用处理的主力是关系型数据库，即SQL面向的是结构化的数据，而Hadoop则针对的是非结构化的数据，从这一角度看，Hadoop提供了对数据处理的一种更为通用的方式。 下面，我们从特定的视角将Hadoop与SQL数据库做详细比较： 1. 用scale-out代替scale-up 拓展商用服务器的代价是非常昂贵的。要运行一个更大的数据库，就要一个更大的服务器，事实上，各服务器厂商往往会把其昂贵的高端机标称为“数据库级服务器”，不过有时候有可能需要处理更大的数据集，但却找不到更大的机器，而更为重要的是，高端机对于许多应用并不经济。 2.用键值对代替关系表 关系型数据库需要将数据按照某种模式存放到具有关系型数据结构表中，但是许多当前的数据模型并不能很好的适应这些模型，如文本、图片、xml等，此外，大型数据集往往是非结构化或半结构化的。而Hadoop以键值对作为最基本的数据单元，能够灵活的处理较少结构化的数据类型。 3.用函数式编程（MapReduce）代替声明式查询（SQL） SQL从根本上说是一个高级声明式语言，它的手段是声明你想要的结果，并让数据库引擎判断如何获取数据。而在MapReduce程序中，实际的数据处理步骤是由你指定的。SQL使用查询语句，而MapReduce使用程序和脚本。MapReduce还可以建立复杂的数据统计模型，或者改变图像数据的处理格式。 4.用离线批量处理代替在线处理 Hadoop并不适合处理那种对几条记录读写的在线事务处理模式，而适合一次写入多次读取的数据需求。 博客内容采摘自：Heaven Wang 的博客：Hadoop详解一：Hadoop简介]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>若泽数据</tag>
        <tag>大数据</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux部署jdk]]></title>
    <url>%2F2019%2F03%2F02%2FLinux%E9%83%A8%E7%BD%B2jdk%2F</url>
    <content type="text"><![CDATA[在官网下载JDK1.8版本并上传到/opt目录 创建目录12mkdir /usr/javamkdir /user/share/java 解压JDK安装包 1tar -zxvf /opt/jdk-8u45-linux-x64.gz -C /usr/java 修改解压后JDK目录权限 1chown -R root:root /usr/java/jdk1.8.0_45/ 配置环境变量 123456echo &quot;&quot;&quot;export JAVA_HOME=/usr/java/jdk1.8.0_45export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$JRE_HOME/lib:$PATH&quot;&quot;&quot; &gt;&gt; /etc/profile 刷新环境变量 1source /etc/profile 验证部署是否成功 12345which java java -version echo $JAVA_HOME]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>基础</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置ssh无密码访问localhost]]></title>
    <url>%2F2019%2F03%2F02%2F%E9%85%8D%E7%BD%AEssh%E6%97%A0%E5%AF%86%E7%A0%81%E8%AE%BF%E9%97%AElocalhost%2F</url>
    <content type="text"><![CDATA[删除默认的.ssh目录 12su - hadooprm -rf ~/.ssh 生成新的ssh配置 1ssk-keygen # 按四次回车 配置信任关系 1cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 修改信任文件权限 1chmod 600 ~/.ssh/authorized_keys 验证 1ssh localhost date # 首次连接需要输入yes，不需输入密码即配置成功]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>大数据</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL基础一]]></title>
    <url>%2F2019%2F01%2F28%2FMySQL%E5%9F%BA%E7%A1%80%E4%B8%80%2F</url>
    <content type="text"><![CDATA[MySQL默认用户及密码修改MySQL数据库的集中登录方式及参数 使用空用户登录1234567891011121314151617[root@hadoop614 ~]# su - mysqladminLast login: Sun Jan 27 18:22:49 CST 2019 on pts/0[mysqladmin@hadoop614 ~]$ mysql -h127.0.0.1Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 5.6.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; exitBye 使用空用户登录12345678910111213141516[mysqladmin@hadoop614 ~]$ mysql -uroot -p -h127.0.0.1Enter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 9Server version: 5.6.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; exitBye 删除空用户，设置root密码12345678910111213141516171819202122232425262728293031323334353637## 查询数据库mysql&gt; show databases;## 切换数据库mysql&gt; use mysql## 查询数据库表mysql&gt; show tables;## 查询表数据mysql&gt; select user,password,host from user；+------+----------+-----------+| user | password | host |+------+----------+-----------+| root | | localhost || root | | hadoop614 || root | | 127.0.0.1 || root | | ::1 || | | localhost || | | hadoop614 |+------+----------+-----------+6 rows in set (0.00 sec)## 删除空用户mysql&gt; delete from user where user = &apos;&apos;;Query OK, 2 rows affected (0.00 sec)## 更新root用户密码mysql&gt; update user set password=password(123456) where user=&apos;root&apos;;Query OK, 4 rows affected (0.00 sec)Rows matched: 4 Changed: 4 Warnings: 0## 查询表数据mysql&gt; select user,password,host from user；+------+-------------------------------------------+-----------+| user | password | host |+------+-------------------------------------------+-----------+| root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | localhost || root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | hadoop614 || root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | 127.0.0.1 || root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | ::1 |+------+-------------------------------------------+-----------+4 rows in set (0.00 sec) #针对用户 权限的操作语句 养成习惯 都最后一步执行刷新权限flush privileges; 12345mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; exitBye 重新登录数据库不输入密码123[mysqladmin@hadoop614 ~]$ mysql -uroot -p -h127.0.0.1Enter password: ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: NO) 正常输入密码123456789101112131415161718[mysqladmin@hadoop614 ~]$ mysql -uroot -p -h127.0.0.1Enter password: ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)[mysqladmin@hadoop614 ~]$ mysql -uroot -p -h127.0.0.1Enter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 16Server version: 5.6.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; 添加用户MySql环境变量1234567891011121314151617[mysqladmin@hadoop614 ~]$ vi .bash_profile# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi# User specific environment and startup programsPATH=$PATH:$HOME/.local/bin:$HOME/bin MYSQL_HOME=/usr/local/mysqlPATH=$&#123;MYSQL_HOME&#125;/bin:$&#123;PATH&#125; export PATHPS1=`uname -n`&quot;:&quot;&apos;$USER&apos;&quot;:&quot;&apos;$PWD&apos;&quot;:&gt;&quot;; export PS1:wq! 刷新环境变量：12[mysqladmin@hadoop614 ~]$ source .bash_profilehadoop614:mysqladmin:/usr/local/mysql:&gt;]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>运维</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署MySQL数据库]]></title>
    <url>%2F2019%2F01%2F28%2F%E9%83%A8%E7%BD%B2MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[部署MySQL数据库的两种方式 一、使用yum方式安装数据库1yum -y install mysql 二、 自定义安装 准备MySQL安装包，上传到linux 查询是否安装过MySQL 123[root@hadoop614 local]# ps -ef|grep mysqld | grep -v grep[root@hadoop614 local]# rpm -qa |grep -i mysql[root@hadoop614 local]# 解压压缩文件 12345[root@hadoop614 local]# tar zvfx mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz [root@hadoop614 local]# mv mysql-5.6.23-linux-glibc2.5-x86_64 mysql[root@hadoop614 local]# ll -d mysql*drwxr-xr-x 13 root root 4096 Jan 27 17:10 mysql-rw-r--r-- 1 root root 311771412 Jan 27 16:18 mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz 创建用户 检查是否存在dba组和mysqladmin用户 123[root@hadoop614 local]# cat /etc/group | grep dba[root@hadoop614 local]# cat /etc/passwd | grep mysqladmin[root@hadoop614 local]# 创建用户组 123[root@hadoop614 local]# groupadd -g 101 dba # -g 制定组id[root@hadoop614 local]# cat /etc/group | grep dbadba:x:101: 创建用户 12345[root@hadoop614 local]# useradd -u 501 -g dba -G root -d /usr/local/mysql mysqladminuseradd: warning: the home directory already exists.# 提示用户家目录已经存在Not copying any file from skel directory into it. # 没有cp skel到家目录中[root@hadoop614 etc]# id mysqladminuid=501(mysqladmin) gid=101(dba) groups=101(dba),0(root) 复制配置文件到mysqladmin家目录 123[root@hadoop614 skel]# cp /etc/skel/.* /usr/local/mysqlcp: omitting directory ‘.’cp: omitting directory ‘..’ 配置my.cnf配置文件 程序查找配置文件顺序 #defualt start: /etc/my.cnf –&gt;/etc/mysql/my.cnf – &gt;SYSCONFDIR/my.cnf-&gt;$MYSQL_HOME/my.cnf-&gt; –defaults-extra-file-&gt;~/my.cnf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103[root@hadoop614 ~]# cd /etc/[root@hadoop614 etc]# vi my.cnf[client]port = 3306socket = /usr/local/mysql/data/mysql.sock[mysqld]port = 3306socket = /usr/local/mysql/data/mysql.sockskip-external-lockingkey_buffer_size = 256Msort_buffer_size = 2Mread_buffer_size = 2Mread_rnd_buffer_size = 4Mquery_cache_size= 32Mmax_allowed_packet = 16Mmyisam_sort_buffer_size=128Mtmp_table_size=32Mtable_open_cache = 512thread_cache_size = 8wait_timeout = 86400interactive_timeout = 86400max_connections = 600# Try number of CPU&apos;s*2 for thread_concurrencythread_concurrency = 32#isolation level and default engine default-storage-engine = INNODBtransaction-isolation = READ-COMMITTEDserver-id = 1basedir = /usr/local/mysqldatadir = /usr/local/mysql/datapid-file = /usr/local/mysql/data/hostname.pid#open performance schemalog-warningssysdate-is-nowbinlog_format = MIXEDlog_bin_trust_function_creators=1log-error = /usr/local/mysql/data/hostname.errlog-bin=/usr/local/mysql/arch/mysql-bin#other logs#general_log =1#general_log_file = /usr/local/mysql/data/general_log.err#slow_query_log=1#slow_query_log_file=/usr/local/mysql/data/slow_log.err#for replication slave#log-slave-updates #sync_binlog = 1#for innodb options innodb_data_home_dir = /usr/local/mysql/data/innodb_data_file_path = ibdata1:500M:autoextendinnodb_log_group_home_dir = /usr/local/mysql/archinnodb_log_files_in_group = 2innodb_log_file_size = 200M#生产上 机械硬盘 sata盘 5000r 7200 10000 15000 ==&gt; ssd 生产# innodb_buffer_pool_size 调大 8Ginnodb_buffer_pool_size = 1024M innodb_additional_mem_pool_size = 50Minnodb_log_buffer_size = 16Minnodb_lock_wait_timeout = 100#innodb_thread_concurrency = 0innodb_flush_log_at_trx_commit = 1innodb_locks_unsafe_for_binlog=1#innodb io features: add for mysql5.5.8performance_schemainnodb_read_io_threads=4innodb-write-io-threads=4innodb-io-capacity=200#purge threads change default(0) to 1 for purgeinnodb_purge_threads=1innodb_use_native_aio=on#case-sensitive file names and separate tablespaceinnodb_file_per_table = 1lower_case_table_names=1[mysqldump]quickmax_allowed_packet = 16M[mysql]no-auto-rehash[mysqlhotcopy]interactive-timeout[myisamchk]key_buffer_size = 256Msort_buffer_size = 256Mread_buffer = 2Mwrite_buffer = 2M 6.修改my.cnf配置文件权限和所属1234[root@hadoop614 etc]# chmod 640 my.cnf[root@hadoop614 etc]# chown mysqladmin:dba my.cnf[root@hadoop614 etc]# ll my.cnf-rw-r----- 1 mysqladmin dba 2326 Jan 27 17:38 my.cnf 7.修改sqladmin家目录权限1234[root@hadoop614 etc]# chmod -R 755 /usr/local/mysql[root@hadoop614 etc]# chown -R mysqladmin:dba /usr/local/mysql[root@hadoop614 etc]# ll -d /usr/local/mysqldrwxr-xr-x 13 mysqladmin dba 4096 Jan 27 17:31 /usr/local/mysql 8.创建arch目录 存储binlog 归档日志 实时同步 MySQL数据到hbasemysql–maxwell–kafka–ss–hbase 12345[root@hadoop614 etc]# su - mysqladmin[mysqladmin@hadoop614 ~]$ pwd/usr/local/mysql[mysqladmin@hadoop614 ~]$ ll -d arch/drwxr-xr-x 2 mysqladmin dba 4096 Jan 27 17:46 arch/ 9.安装MySQL数据库1234567[root@hadoop614 etc]# su - mysqladmin[mysqladmin@hadoop614 ~]$ pwd/usr/local/mysql[mysqladmin@hadoop614 ~]$ scripts/mysql_install_db \&gt; --user=mysqladmin \&gt; --basedir=/usr/local/mysql \&gt; --datadir=/usr/local/mysql/data 10.配置MySQL服务并引导自动启动1234567891011121314[mysqladmin@hadoop614 support-files]$ exitlogout# 将服务文件拷贝到init.d下，并重命名为mysql[root@hadoop614 ~]# cd /usr/local/mysql[root@hadoop614 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql #赋予可执行权限[root@hadoop614 mysql]# chmod +x /etc/rc.d/init.d/mysql#删除服务[root@hadoop614 mysql]# chkconfig --del mysql[#添加服务root@hadoop614 mysql]# chkconfig --add mysql[root@hadoop614 mysql]# chkconfig --level 345 mysql on[root@hadoop614 mysql]# vi /etc/rc.local在文件中追加：su - mysqladmin -c &quot;/etc/init.d/mysql start --federated&quot; 11.启动MySQL并查看进程和侦听12345678910111213141516171819[root@hadoop614 mysql]# su - mysqladminLast login: Sun Jan 27 18:04:31 CST 2019 on pts/1[mysqladmin@hadoop614 ~]$ rm -rf my.cnf方法一：[mysqladmin@hadoop614 ~]$ service mysql startStarting MySQL..[ OK ][mysqladmin@hadoop614 ~]$ service mysql statusMySQL running (14023)[ OK ]方法二：[mysqladmin@hadoop614 ~]$ service mysql stopShutting down MySQL..[ OK ][mysqladmin@hadoop614 ~]$ mysqld_safe &amp;[1] 14901[mysqladmin@hadoop614 ~]$ 190127 18:24:18 mysqld_safe Logging to &apos;/usr/local/mysql/data/hostname.err&apos;.190127 18:24:18 mysqld_safe Starting mysqld daemon with databases from /usr/local/mysql/data[mysqladmin@hadoop614 ~]$ service mysql statusMySQL running (15544)[ OK ][mysqladmin@hadoop614 ~]$ 到这里，MySQL数据库已部署完成。 部署过程中MySQL默认创建两个用户一：root用户，密码为空二：空用户，空密码]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>运维</tag>
        <tag>Linux基础</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础命令三]]></title>
    <url>%2F2019%2F01%2F26%2FLinux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E4%B8%89%2F</url>
    <content type="text"><![CDATA[学习Linux基础命令第三天 文本编辑器：vi 注：在生产环境/操作系统上直接编辑文本文件时，一定要先进行备份，再对源文件进行编辑修改操作。例：需要修改用户环境变量时： 1[root@hadoop614 ~]# cp .bash_profile .bash_profile_20190124 ## 编辑器常用命令跳转到第一行首字符：gg跳转到左后一行首字符：G跳转到行尾：shift $跳转到行首：shift ^删除当前行：dd删除光标以下的所有行：dG删除光标一下n行：ndd 场景一：清楚文件内所有内容1234567方法一:[root@hadoop614 ~]# ll test.txt -rw-r--r-- 1 root root 42 Jan 26 15:08 test.txt #查看文件大小 42 字节[root@hadoop614 ~]# echo &gt; test.txt # 像文件内重定向一个空值替换文件所有内容[root@hadoop614 ~]# ll test.txt -rw-r--r-- 1 root root 1 Jan 26 15:09 test.txt # 查看文件大小为 1字节结论：文件大小还有1字节，判断时为非空文件！因echo输出本身自带换行符 1234567方法二:[root@hadoop614 ~]# ll test.txt -rw-r--r-- 1 root root 42 Jan 26 15:08 test.txt #查看文件大小 42 字节[root@hadoop614 ~]# cat /dev/null &gt; test.txt # 将/dev/null内容输出到3.txt文件内，/dev/null为纯空[root@hadoop614 ~]# ll test.txt -rw-r--r-- 1 root root 0 Jan 26 15:19 test.txt结论：文件大小为0字节，判断时为空文件！因cat /dev/null返回无任何内容，无换行符或者空格等 场景二：在文件最后追加内容123456789101112[root@hadoop614 ~]# vi test.txt 光标切换到最后一行：`G`光标切换到最后一个字符：`$`方法一：进入编辑模式：`i` #当前光标位置进行编辑光标像后移动一位：`→`回车进入下一行方法二：进入编辑模式：`a` # 当前光标位置后一位进行编辑回车进入下一行方法三：进入编辑模式：`o` # 当前光标位置下一行新建一行进行编辑 场景三：快速定位关键字 - error123456789101112131415方法一：[root@hadoop614 ~]# vi test.txt 在尾行模式输入/error，回车即可向下查找或者光标在文件任意位置，可输入?error，回车即可向上查找注：确为当前光标位置：尾行模式输入 `:set nu` 可显示行号尾行模式输入 `:set nonu` 可取消显示行号方法二：# 可一次性输出文件内所有存在关键字的所有行[root@hadoop614 ~]# cat test.txt | grep errorerror 1 2 3error 4error方法三：将test.txt文件下载到本地，使用PadNotes++等文本编辑工具打开并查找 更多vi编辑器基础知识【Linux基础】VIM编辑器常用命令详解（基础篇） 权限控制权限解读123[root@hadoop614 ~]# su - hadoop[hadoop@hadoop614 ~]$ ll /rootls: cannot open directory /root: Permission denied # Permission denied 表示权限不足或者没有权限 1234567891011121314# 切换回root用户，看看为什么hadoop用户无权访问/root[root@hadoop614 ~]# ll -d /rootdr-xr-x---. 6 root root 4096 Jan 26 15:42 /root# ll命令查询出的结果介绍：/root # 文件或目录名Jan 26 15:42 # 表示文件/目录创建时间或文件更新时间4096 # 表示文件大小和目录本身的大小，一般情况下目录统一为4096，因为这个值不包含目录下的文件和子目录内文件的大小root root # 第一个root表示文件或目录所属用户--所属者 # 第二个root表示文件或目录所属组--所属组dr-xr-x--- # 第1位：d:表示目录 -:表示文件 l:表示软硬链接(软链接类似于快捷方式) # 第2-4位：r-x：表示目录或文件所属者的权限 # 第5-7位：r-x：表示目录或文件所属组成员用户的权限 # 第8-10位：---：表示其他用户对目录或文件的权限 权限说明： -：表示无权限 – 0x：表示执行权限 – 1w：表示可写权限 – 2r：表示刻度权限 –4解读/root目录权限：550dr-xr-x—. 6 root root 4096 Jan 26 15:42 /root所属者：root 拥有权限：读+执行 = 4+1 = 5所属组：root 拥有权限：读+执行 = 4+1 = 5其他用户拥有权限：无 = 0 权限修改修改文件或目录权限：chmod12345[root@hadoop614 ~]# ll test.txt-rw-r--r-- 1 root root 295 Jan 26 15:42 test.txt # test.txt文件权限为644[root@hadoop614 ~]# chmod 755 test.txt[root@hadoop614 ~]# ll test.txt -rwxr-xr-x 1 root root 295 Jan 26 15:42 test.txt # test.txt文件权限为755 若要一起修改目录及其子文件和子目录的权限，则使用chmod -R参数 修改文件或目录所属者和所属组：chown123[root@hadoop614 ~]# chown hadoop:hadoop001 test.txt [root@hadoop614 ~]# ll test.txt-rwxr-xr-x 1 hadoop hadoop001 295 Jan 26 15:42 test.txt # 此时文件所属以被修改为hadoop:hadoop001(所属者:所属组) 若要一起修改目录及其子文件和子目录的所属，则使用chown -R参数 执行shell脚本 shell脚本也是文本文件，一般命名后缀为.sh 想要文件可执行，首先需要赋予文件可执行权限12[root@hadoop614 ~]# ll bigdata.sh -rw-r--r-- 1 root root 66 Jan 26 16:34 bigdata.sh # 文件权限644 没有执行权限 执行脚本的两种方法方法一：./bigdata.sh方法二：sh bigdata.sh1234567[root@hadoop614 ~]# ./bigdata.sh-bash: ./bigdata.sh: Permission denied # 没有执行权限是会返回权限不足[root@hadoop614 ~]# chmod u+x bigdata.sh # 给所属者添加执行权限[root@hadoop614 ~]# ll bigdata.sh -rwxr--r-- 1 root root 23 Jan 26 16:49 bigdata.sh[root@hadoop614 ~]# ./bigdata.sh 若泽大数据 软链接 软连接：相当于Windows系统中的快捷方式，删除软连接而源文件删除，但通过软连接对文件进行编辑后，源文件也会修改 创建软连接：ln -s12345678[root@hadoop614 ~]# ln -s bigdata.sh bat[root@hadoop614 ~]# lltotal 4lrwxrwxrwx 1 root root 10 Jan 26 16:56 bat -&gt; bigdata.sh-rwxr--r-- 1 root root 36 Jan 26 16:51 bigdata.sh[root@hadoop614 ~]# cat bat#!/bin/bash echo &quot;若泽大数据&quot; 远程上传/下载：rz/sz12# 安装传输工具：[root@hadoop614 ~]# yum -y install lrzsz 上传文件：rz选择需要上传的文件 –&gt; 点击打开上传完成 下载文件：sz[root@hadoop614 ~]# sz bigdata.shbat文件下载完成 系统命令系统资源使用了监控：top 可监控系统资源使用率情况通常情况下，生产环境load average: 0.00, 0.01, 0.05经验值不可超过10&lt;【Linux基础】top命令的用法详细详解&gt; 12345678910111213141516top - 17:19:46 up 5 days, 10:10, 1 user, load average: 0.00, 0.01, 0.05Tasks: 83 total, 1 running, 82 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.0 us, 0.2 sy, 0.0 ni, 99.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 3880412 total, 1854140 free, 116716 used, 1909556 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 3456572 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1377 root 0 -20 126352 11548 8920 S 0.3 0.3 13:03.91 AliYunDun 1 root 20 0 43508 3744 2448 S 0.0 0.1 0:25.31 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.11 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 6 root 20 0 0 0 0 S 0.0 0.0 0:00.16 kworker/u4:0 7 root rt 0 0 0 0 S 0.0 0.0 0:00.04 migration/0 8 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_bh 9 root 20 0 0 0 0 S 0.0 0.0 0:39.26 rcu_sched 查询内存使用情况：free -m1234[root@hadoop614 ~]# free -m total used free shared buff/cache availableMem: 3789 114 1810 0 1864 3375Swap: 0 0 0 文件系统查询：df12345678910[root@hadoop614 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 4.8G 33G 13% /devtmpfs 1.9G 0 1.9G 0% /devtmpfs 1.9G 0 1.9G 0% /dev/shmtmpfs 1.9G 592K 1.9G 1% /runtmpfs 1.9G 0 1.9G 0% /sys/fs/cgroupoverlay 40G 4.8G 33G 13% /var/lib/docker/overlay2/20cf413aad752a7988a0667d87698f5dcb6677329bc34184ad0c148950ada89a/mergedshm 64M 0 64M 0% /var/lib/docker/containers/0f9bcc7a9ebb896384bc444019fd4b45aca9a9e63d4b7057c6f1a4a6e1621a35/shmtmpfs 379M 0 379M 0% /run/user/0 压缩与解压缩：zip/unzip gzip/gunzip1234# 压缩需要安装压缩插件[root@hadoop614 ~]# zip bigdata.sh -bash: zip: command not found[root@hadoop614 ~]# yum -y install zip 123456# 压缩为zip文件，源文件不变，重新生成zip压缩文件[root@hadoop614 ~]# zip -r bigdata.zip bigdata.sh adding: bigdata.sh (stored 0%)[root@hadoop614 ~]# ll bigdata.zip bigdata.sh -rwxr--r-- 1 root root 36 Jan 26 16:51 bigdata.sh-rw-r--r-- 1 root root 206 Jan 26 17:37 bigdata.zip 12345# 压缩为gz文件，源文件删除[root@hadoop614 ~]# gzip bigdata.sh[root@hadoop614 ~]# ll bigdata.*-rwxr--r-- 1 root root 68 Jan 26 16:51 bigdata.sh.gz-rw-r--r-- 1 root root 206 Jan 26 17:37 bigdata.zip 1234# 解压缩需要安装解压缩插件[root@hadoop614 ~]# unzip bigdata.zip -bash: unzip: command not found[root@hadoop614 ~]# yum -y install unzip 12345678# 解压zip文件：解压缩后压缩文件和源文件并存[root@hadoop614 ~]# unzip bigdata.zip Archive: bigdata.zip extracting: bigdata.sh [root@hadoop614 ~]# ll bigdata.*-rwxr--r-- 1 root root 36 Jan 26 16:51 bigdata.sh-rwxr--r-- 1 root root 68 Jan 26 16:51 bigdata.sh.gz-rw-r--r-- 1 root root 206 Jan 26 17:37 bigdata.zip 123456# 解压缩gz文件：解压缩后gz文件消耗四，仅保留解压后的文件[root@hadoop614 ~]# gunzip bigdata.sh.gz gzip: bigdata.sh already exists; do you wish to overwrite (y or n)? y[root@hadoop614 ~]# ll bigdata.*-rwxr--r-- 1 root root 36 Jan 26 16:51 bigdata.sh-rw-r--r-- 1 root root 206 Jan 26 17:37 bigdata.zip 小结：zip/unzip 压缩和解压缩后，源文件和压缩文件并存gzip/gunzip 压缩后只有压缩后的文件，解压缩后只有解压缩后的文件 打包压缩多个文件到同一个包内:tar1234567891011121314151617压缩：[root@hadoop614 ~]# tar -czvf test.tar.gz test*test1test2test3[root@hadoop614 ~]# ll test*-rw-r--r-- 1 root root 26 Jan 26 17:48 test1.gz-rw-r--r-- 1 root root 26 Jan 26 17:48 test2.gz-rw-r--r-- 1 root root 26 Jan 26 17:48 test3.gz-rw-r--r-- 1 root root 129 Jan 26 17:49 test.tar.gz解压缩：[root@hadoop614 ~]# tar -xzvf test.tar.gz[root@hadoop614 ~]# ll test*-rw-r--r-- 1 root root 26 Jan 26 17:48 test1.gz-rw-r--r-- 1 root root 26 Jan 26 17:48 test2.gz-rw-r--r-- 1 root root 26 Jan 26 17:48 test3.gz-rw-r--r-- 1 root root 129 Jan 26 17:49 test.tar.gz 下载URL内容：wget 用法：wget URL 1234567891011121314151617181920212223242526272829[root@hadoop614 test]# wget https://mp.csdn.net/postlist--2019-01-26 17:59:00-- https://mp.csdn.net/postlistResolving mp.csdn.net (mp.csdn.net)... 47.95.47.253Connecting to mp.csdn.net (mp.csdn.net)|47.95.47.253|:443... connected.HTTP request sent, awaiting response... 307 Temporary RedirectLocation: http://passport.csdn.net/account/login?from=https://mp.csdn.net/postlist [following]--2019-01-26 17:59:00-- http://passport.csdn.net/account/login?from=https://mp.csdn.net/postlistResolving passport.csdn.net (passport.csdn.net)... 101.201.169.146Connecting to passport.csdn.net (passport.csdn.net)|101.201.169.146|:80... connected.HTTP request sent, awaiting response... 301 Moved PermanentlyLocation: https://passport.csdn.net/account/login?from=https://mp.csdn.net/postlist [following]--2019-01-26 17:59:00-- https://passport.csdn.net/account/login?from=https://mp.csdn.net/postlistConnecting to passport.csdn.net (passport.csdn.net)|101.201.169.146|:443... connected.HTTP request sent, awaiting response... 302 Location: https://passport.csdn.net/login [following]--2019-01-26 17:59:00-- https://passport.csdn.net/loginReusing existing connection to passport.csdn.net:443.HTTP request sent, awaiting response... 200 OKLength: 1121 (1.1K) [text/html]Saving to: ‘postlist’100%[==============================================================================&gt;] 1,121 --.-K/s in 0s 2019-01-26 17:59:00 (211 MB/s) - ‘postlist’ saved [1121/1121][root@hadoop614 test]# cat postlist&lt;!DOCTYPE html&gt;&lt;html translate=no lang=zh-CN&gt;&lt;head&gt;&lt;meta http-equiv=X-UA-Compatible content=&quot;IE=Edge,chrome=1&quot;&gt;&lt;meta name=referrer content=always&gt;&lt;meta name=renderer content=webkit&gt;&lt;meta name=force-rendering content=webkit&gt;&lt;meta charset=utf-8&gt;&lt;meta name=description content=CSDN登录注册&gt;&lt;meta name=google value=notranslate&gt;&lt;link type=image/x-icon href=https://csdnimg.cn/public/favicon.ico rel=&quot;SHORTCUT ICON&quot;&gt;&lt;title&gt;CSDN-专业IT技术社区&lt;/title&gt;&lt;!--[if lt IE 9]&gt; &lt;script&gt;window.location.href=&quot;https://g.csdnimg.cn/browser_upgrade/1.0.2/index.html&quot;;&lt;/script&gt; &lt;![endif]--&gt;&lt;link href=https://csdnimg.cn/release/passport_fe/assets/css/login.abab6048cf6e272a47f28638f3aa52ec.css rel=stylesheet&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=app&gt;&lt;/div&gt;&lt;script type=text/javascript src=https://csdnimg.cn/release/passport_fe/assets/js/manifest.603a92bc4219ee818e42.js&gt;&lt;/script&gt;&lt;script type=text/javascript src=https://csdnimg.cn/release/passport_fe/assets/js/vendor.020317dbffa47ab1eb8b.js&gt;&lt;/script&gt;&lt;script type=text/javascript src=https://csdnimg.cn/release/passport_fe/assets/js/login.5853170f9532597487d3.js&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 调度任务/计划任务：crontab 编辑创建任务：crontab -e # vi编辑模式查询任务内容：crontab -l 编辑脚本内容为：1234567[root@hadoop614 ~]# cat bat#!/bin/bash for((i=1;i&lt;=6;i++));do date sleep 10sdone 12[root@hadoop614 ~]# crontab -l* * * * * /root/bat &gt;&gt; /root/bat.log # 书写规则： 分 时 日 月 周 执行的命令 * 表示’每‘ 1234[root@hadoop614 ~]# tail -F bat.log Sat Jan 26 18:14:01 CST 2019Sat Jan 26 18:14:11 CST 2019Sat Jan 26 18:14:21 CST 2019 后台执行命令/脚本：nohup xxxx &amp;前台执行结果：1[root@hadoop614 ~]# ./bat &gt;&gt; bat.log 后台执行结果12345678[root@hadoop614 ~]# nohup ./bat &gt;&gt; bat.log 2&gt;&amp;1 &amp;[2] 10691[root@hadoop614 ~]# ps -ef | grep 10691 | grep -v greproot 10691 10335 0 18:18 pts/0 00:00:00 /bin/bash ./batroot 10703 10691 0 18:18 pts/0 00:00:00 sleep 10s[root@hadoop614 ~]# kill -9 10691[root@hadoop614 ~]# [2]+ Killed nohup ./bat &gt;&gt; bat.log 2&gt;&amp;1 详细的输出重定向介绍：【若泽大数据第一天】基础-Linux基础命令一]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>运维</tag>
        <tag>Linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础命令二]]></title>
    <url>%2F2019%2F01%2F23%2FLinux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[学习Linux基础命令第二天 Linux基础命令二 查询历史命令记录：history 历史执行记录保存在 ~/.bash_historty文件中 123456[root@hadoop614 ~]# history 1 echo &gt;.bash_history 2 history 3 vi .bash_history 4 exit[root@hadoop614 ~]# !3 # 执行history命令中第三条指令 用户和组： 创建用户：useradd删除用户：userdel编辑用户：usermod创建组：groupadd删除组：groupdel编辑组：groupmod 123[root@hadoop614 ~]# useradd ruoze[root@hadoop614 ~]# id ruozeuid=1001(ruoze) gid=1002(ruoze) groups=1002(ruoze) 123[root@hadoop614 ~]# groupadd data[root@hadoop614 ~]# cat /etc/group | grep datadata:x:1003: 1234# 一个用户可以属于多个组，但必须有切只有一个主组[root@hadoop614 ~]# usermod -a -G data ruoze[root@hadoop614 ~]# id ruozeuid=1001(ruoze) gid=1002(ruoze) groups=1002(ruoze),1003(data) 1234## 同一个组可以有多个用户[root@hadoop614 ~]# usermod -a -G hadoop001 ruoze[root@hadoop614 ~]# cat /etc/group | grep hadoop001hadoop001:x:1001:hadoop,ruoze 1234## 用户删除后，主组内无其他用户，则主组自动删除[root@hadoop614 ~]# userdel ruoze[root@hadoop614 ~]# id ruozeid: ruoze: no such user 123[root@hadoop614 ~]# groupdel data[root@hadoop614 ~]# cat /etc/group | grep data[root@hadoop614 ~]# 设置密码：passwd 1234567# passwd 后制定要修改密码的用户，不指定则默认为当前用户[root@hadoop614 ~]# passwd ruozeChanging password for user ruoze.New password: BAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word Retype new password: passwd: all authentication tokens updated successfully. 切换用户：su 123[root@hadoop614 ~]# su - ruoze # - 在切换用户时加载用户环境变量并切换到用户家目录[ruoze@hadoop614 ~]$ pwd/home/ruoze 123[root@hadoop614 ~]# su ruoze # 不加-，在切换用户时仅加载用户环境变量[ruoze@hadoop614 root]$ pwd/root 普通用户临时获取root权限：sudo12345[root@hadoop614 ~]# vi /etc/sudoers添加修改一下内容## Allow root to run any commands anywhereroot ALL=(ALL) ALLruoze ALL=(root) NOPASSWD:ALL 1234567[root@hadoop614 ~]# su - ruozeLast login: Wed Jan 23 14:34:57 CST 2019 on pts/0[ruoze@hadoop614 ~]$ ll /rootls: cannot open directory /root: Permission denied[ruoze@hadoop614 ~]$ sudo ls -lrt /roottotal 4drwxr-xr-x 2 root root 4096 Jan 20 13:10 3 筛选/过滤：grep 管道/过滤：| 123# 将 | 前的执行结果作为后面的执行输入[root@hadoop614 ~]# cat /etc/passwd | grep ruoze # 在 | 前的执行结果中过滤出包含‘ruoze’的一行数据ruoze:x:1001:1002::/home/ruoze:/bin/bash 退出：exit 查看进程：ps 12345[root@hadoop614 ~]# ps -ef | grep sshdroot 3292 1 0 Jan21 ? 00:00:00 /usr/sbin/sshd -D # ssh程序进程root 7341 3292 0 14:25 ? 00:00:00 sshd: root@pts/0 # root 用户登录进程root 7492 7343 0 14:44 pts/0 00:00:00 grep --color=auto sshd # grep 过滤进程#进程所属用户 pid ppid 日期时间 进程名 杀死进程：kill 【Linux基础命令】kill详解 12# 前面的先不看，就是输出tail进程的pid，kill -9 进行杀死进行[root@hadoop614 ~]# ps -ef | grep tail | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9 查看进程的端口号：netstat 12345# 不是所有的进程都有端口号[root@hadoop614 ~]# ps -ef | grep tail | grep -v grep | awk &apos;&#123;print $2&#125;&apos; 7562[root@hadoop614 ~]# netstat -nlp | grep 7562 # tail命令的进程没有端口号[root@hadoop614 ~]# 12345[root@hadoop614 ~]# ps -ef | grep /usr/sbin/sshd | grep -v grep | awk &apos;&#123;print $2&#125;&apos;3292[root@hadoop614 ~]# netstat -nlp | grep 3292 # ssh服务可查询到端口tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 3292/sshd # 协议 提供服务的地址:端口 允许访问地址段：端口 监听 拒绝错误 Connection refused：1、查看服务器应用进程是否存在: ps -ef | grep xxxx2、查看进程端口是否正确: netstat -nlp | grep $pid3、查看端口允许访问的服务器地址段 搜索：find123# find 目录 -name &apos;*名字*&apos;[root@hadoop614 ~]# find ./ -name ruoz./3/ruoz 安装软件包：yum12345678910[root@hadoop614 ~]# yum search http # 查看http属于哪一个安装包[root@hadoop614 ~]# yum install -y httpd #下载并安装rpm安装包[root@hadoop614 ~]# service httpd restart # 重新启动httpd服务Redirecting to /bin/systemctl restart httpd.service[root@hadoop614 ~]# rpm -qa|grep httpd # 查看httpd版本包名称httpd-tools-2.4.6-88.el7.centos.x86_64httpd-2.4.6-88.el7.centos.x86_64[root@hadoop614 ~]# rpm -e httpd-2.4.6-88.el7.centos.x86_64 # 删除/卸载httpd版本程序[root@hadoop614 ~]# rpm -qa|grep httpdhttpd-tools-2.4.6-88.el7.centos.x86_64 123456789# 删除/卸载使可能出现一下错误[root@hadoop001 ~]# rpm -e httpd-2.2.15-69.el6.centos.x86_64error: Failed dependencies: httpd &gt;= 2.2.0 is needed by (installed) gnome-user-share-2.28.2-3.el6.x86_64# 使用--nodeps参数跳过版本关联检测[root@hadoop001 ~]# rpm -e --nodeps httpd-2.2.15-69.el6.centos.x86_64warning: /etc/httpd/conf/httpd.conf saved as /etc/httpd/conf/httpd.conf.rpmsave[root@hadoop001 ~]# rpm -qa|grep httpdhttpd-tools-2.2.15-69.el6.centos.x86_64 查找命令的可执行文件位置：which 说明：which命令使从用户的环境变量$PATH中逐层查找 123[root@hadoop614 ~]# which lsalias ls=&apos;ls --color=auto&apos; /usr/bin/ls 查找命令所在的相关路径：whereis 12[root@hadoop614 ~]# whereis lsls: /usr/bin/ls /usr/share/man/man1/ls.1.gz 帮助与详情：--help 和 man 使用方法:ls –help #命令 –helpman ls # man 命令]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>运维</tag>
        <tag>Linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VIM编辑器常用命令详解（基础篇）]]></title>
    <url>%2F2019%2F01%2F23%2FVIM%E7%BC%96%E8%BE%91%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9F%BA%E7%A1%80%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、vi/vim是什么Linux世界几乎所有的配置文件都是以纯文本形式存在的，而在所有的Linux发行版系统上都有vi编辑器，因此利用简单的文字编辑软件就能够轻松地修改系统的各种配置了，非常方便。vi就是一种功能强大的文本编辑器，而vim则是高级版的vi，不但可以用不同颜色显示文字内容，还能进行诸如shell脚本、C语言程序编辑等功能，可以作为程序编辑器。 二、为什么要学习vi/vim？ 首先所有的Linux发行版系统上都会默认内置vi编辑器，而不一定带有其他文本编辑器，非常通用；其次，很多软件的编辑接口都会默认调用vi；第三，vi具有程序编辑的能力；最后，vi程序简单，编辑速度相当快速。\ 三、vi的三种模式及各个模式之间的转换关系 四、常用快捷方式1.光标移动 按键 光标动作 h j k l , ← ↓ ↑ → 移动一位，h j k l 代表左、下、上、右 数字 0 移至本行开头 ^ 移至本行第一个非空字符，匹配开头 $ 移至本行结尾，可以包含空格 w 移至下一单词或标点的开头 W 移至下一单词开头，忽略标点 b 移至上一单词或标点开头 B 移至上一单词开头，忽略标点 ctrl-f/PgDn 下翻一页 ctrl-b/PgUp 上翻一页 nG 移至第n行 G 移至光标最后一行 : n enter键 移至第n行 n+ 向下跳n行 n- 向上跳n行 H 移至当前屏幕的第一行 L 移至当前屏幕的最后一行 注：许多vi的命令前面都可以缀上数字，前缀数字可以控制该命令执行的次数，比如5j可以使得光标向下移动5行。 2.基本编辑 按键 光标动作 a 在当前字符后插入文本 A 在行尾插入文本 i 在当前字符前插入文本 I 在行首插入文本 o 当前行下方插入新行并进入插入模式 O 当前行上方插入新行并进入插入模式 3. 删除和撤销 按键 光标动作 x 删除当前字符（剪切） nx 向后删除当前行在内的n个字符（剪切） dd 删除（剪切）当前行 ndd 向下删除当前行在内的n行（剪切） dW 删除当前字符到下一单词的起始处（删除整个单词） d$ 删除当前字符到当前行的末尾（剪切） d0 删除当前字符到当前行的起始处（剪切） d^ 删除当前字符当当前行下一个非空字符（剪切） dG 删除当前行到文件末尾（剪切） d20G 删除当前行到文件第20行（d与定位符结合使用，x不行）（剪切） u 撤销上一次操作 nu 撤销n次操作 U 撤销对当前行的所有操作 4.剪切、复制、粘贴 注意：x和d键实质上是剪切键，两者功能略有差异。y是复制键，p是粘贴键。 键 光标动作 yy 复制当前行 5yy 向下复制当前行在内的5行 yw 当前字符到下一单词的起始处 y$ 当前字符到当前行的末尾 y^ 当前字符到当前行下一个非空字符 y0 当前字符到当前行的行首 yG 当前行到文件末尾 y20G 当前行到文件第20行（用法和d完全一样，d实际就是剪切） p 粘贴到当前字符（行）后面（下方） P 粘贴到当前字符（行）前面（上方） J 合并当前行和下一行为一行 R 替换模式，和windows下的insert模式差不多 v 进入选择模式（VISUAL），可以利用方向键选择字符，然后x、d剪切或者y复制 5.查找和替换 键 光标动作 fx（当前行内查找） 光标移至本行下一处出现字符x的位置，输入分号重复上一次搜索 /（搜索单词或短语） 使用/命令后，屏幕底端会出现/，接下来输入要搜索的单词或短语，enter结束 : s/old/new 用new替换行中首次出现的old（:分号用于启动一条ex命令） : s/old/new/g 用new替换行中所有的old :n,m s/old/new/g 用new替换从n到m行里所有的old :%s/old/new/g 用new替换当前文件里所有的old :set nu 显示行号 :set nonu 取消显示行号 6.保存工作 键 光标动作 :w 仅保存文件 :q 退出文件，没有修改 :q! 放弃修改并退出文件 :wq 保存并退出文件 :w file1 文件另存为file1 ZZ 保存并退出文件]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>运维</tag>
        <tag>Linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kill详解]]></title>
    <url>%2F2019%2F01%2F23%2Fkill%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简单描述 Linux 系统的 kill 命令 1．命令格式： kill [参数] [进程号] 2．命令功能： 发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用“-KILL” 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。 3．命令参数： 参数 参数介绍 -l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称 -a 当处理当前进程时，不限制命令名和进程号的对应关系 -p 指定kill 命令只打印相关进程的进程号，而不发送任何信号 -s 指定发送信号 -u 指定用户 注意：1、kill命令可以带信号号码选项，也可以不带。如果没有信号号码，kill命令就会发出终止信号(15)，这个信号可以被进程捕获，使得进程在退出之前可以清理并释放资源。也可以用kill向进程发送特定的信号。例如：kill -2 123它的效果等同于在前台运行PID为123的进程时按下Ctrl+C键。但是，普通用户只能使用不带signal参数的kill命令或最多使用-9信号。2、kill可以带有进程ID号作为参数。当用kill向这些进程发送信号时，必须是这些进程的主人。如果试图撤销一个没有权限撤销的进程或撤销一个不存在的进程，就会得到一个错误信息。3、可以向多个进程发信号或终止它们。4、当kill成功地发送了信号后，shell会在屏幕上显示出进程的终止信息。有时这个信息不会马上显示，只有当按下Enter键使shell的命令提示符再次出现时，才会显示出来。5、应注意，信号使进程强行终止，这常会带来一些副作用，如数据丢失或者终端无法恢复到正常状态。发送信号时必须小心，只有在万不得已时，才用kill信号(9)，因为进程不能首先捕获它。要撤销所有的后台作业，可以输入kill 0。因为有些在后台运行的命令会启动多个进程，跟踪并找到所有要杀掉的进程的PID是件很麻烦的事。这时，使用kill 0来终止所有由当前shell启动的进程，是个有效的方法。 4．使用实例： 实例1：列出所有信号名称命令：kill -l1234567891011121314[root@hadoop614 ~]# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX ==本人平时只是用kill -9的参数，百度看到飘飘雪的内容很详细，内容均来自于飘飘雪博客==博客园–飘飘雪(linux kill命令详解)]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>运维</tag>
        <tag>Linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础命令一]]></title>
    <url>%2F2019%2F01%2F20%2FLinux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Linux 基础命令是作为运维及大数据开发人员必备技能之一 Linux系统用户12[root@hadoop614 ~]# iduid=0(root) gid=0(root) groups=0(root) # root为unix系统超级管理员用户 Linux基础命令 查看当前所在目录：pwd 12[root@hadoop614 ~]# pwd/root # /root为当前所在目录 切换所在目录：cd 1234567[root@hadoop614 ~]# cd /etc # / 表示根目录,为Linux系统一级目录 # etc为根目录下的二级目录 # ~为表示用户家目录 [root@hadoop614 etc]# # ‘#’表示用当前用户为管理员用户 # etc位置表示当前所在目录[root@hadoop614 etc]# cd # 切换到用户家目录[root@hadoop614 ~]# cd ~ # 切换到用户家目录 查看所在目录下内容：ls 12345678[root@hadoop614 ~]# ls # 查询当前目录下所有显性文件和目录名称1.txt 2.sh 3.ini 4.py [root@hadoop614 ~]# ls -l # -l :等同于‘ll’;显示当前目录内容及其属性 total 0 -rw-r--r-- 1 root root 0 Jan 20 12:34 1.txt # -rw-r--r-- 表示文件或文件夹权限 -rw-r--r-- 1 root root 0 Jan 20 12:34 2.sh # root表示文件或文件夹所属用户 -rw-r--r-- 1 root root 0 Jan 20 12:34 3.ini # root表示文件或文件夹所属组 -rw-r--r-- 1 root root 0 Jan 20 12:34 4.py # 后面依次为文件大小，更新月 日期 时间 名字 12345678910[root@hadoop614 ~]# ll -a # -a 表示查询当前目录下所有文件和目录名称(包含隐藏文件) # 隐藏文件/目录：以 . 开头的文件或目录total 48dr-xr-x---. 5 root root 4096 Jan 20 12:34 . #表示当前目录dr-xr-xr-x. 18 root root 4096 Jan 15 21:33 .. #表示父目录/上一级目录-rw------- 1 root root 321 Jan 16 23:20 .bash_history #用户历史输入命令记录文件-rw-r--r--. 1 root root 18 Dec 29 2013 .bash_logout # 用户登录记录文件-rw-r--r--. 1 root root 176 Dec 29 2013 .bash_profile # 用户环境变量配置文件-rw-r--r--. 1 root root 176 Dec 29 2013 .bashrc # 用户环境变量配置文件····· #内容过多，不做展示 123456[root@hadoop614 ~]# ll -rt # r 表示正序排序 t 表示时间 ：按时间正序排序total 8-rw-r--r-- 1 root root 0 Jan 20 12:34 4.py-rw-r--r-- 1 root root 0 Jan 20 12:34 3.ini-rw-r--r-- 1 root root 61 Jan 20 12:47 1.txt-rw-r--r-- 1 root root 2 Jan 20 12:48 2.sh 1234[root@hadoop614 ~]# ll -h #提升文件或目录大小的可读性total 660K-rw-r--r-- 1 root root 61 Jan 20 12:47 1.txt-rw-r--r-- 1 root root 655K Jan 20 12:50 services 清空当前屏幕：clear 查看当前系统网卡IP地址：ifconfig Linux : ifconfigWindows : ipconfigunix/AIX : ifconifg -a 123456789101112131415[root@hadoop614 ~]# ifconfigeth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.17.64.32 netmask 255.255.240.0 broadcast 172.17.79.255 ether 00:16:3e:0e:05:61 txqueuelen 1000 (Ethernet) RX packets 93473 bytes 8748270 (8.3 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 77688 bytes 8952210 (8.5 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 创建目录 ：mkdir123456[root@hadoop614 ~]# mkdir abc # 创建目录名为 abc[root@hadoop614 ~]# lltotal 664-rw-r--r-- 1 root root 61 Jan 20 12:47 1.txt # -rw-r--r-- 第一个‘-’表示文件drwxr-xr-x 2 root root 4096 Jan 20 13:03 abc # drwxr-xr-x 中的’d&apos;表示目录directory-rw-r--r-- 1 root root 670293 Jan 20 12:50 services 1234[root@hadoop614 ~]# mkdir -p a/b/c # -p：表示若创建目标目录的父目录不存在则创建父目录[root@hadoop614 ~]# ll a/btotal 4drwxr-xr-x 2 root root 4096 Jan 20 13:08 c 123456[root@hadoop614 ~]# mkdir 1 2 3 # 同时创建多个目录，目录名间以空格区分[root@hadoop614 ~]# lltotal 12drwxr-xr-x 2 root root 4096 Jan 20 13:10 1drwxr-xr-x 2 root root 4096 Jan 20 13:10 2drwxr-xr-x 2 root root 4096 Jan 20 13:10 3 创建文件：touch #创建空文件 touch + 文件名 1234567 [root@hadoop614 ~]# touch ruozedata.txt [root@hadoop614 ~]# lltotal 12drwxr-xr-x 2 root root 4096 Jan 20 13:10 1drwxr-xr-x 2 root root 4096 Jan 20 13:10 2drwxr-xr-x 2 root root 4096 Jan 20 13:10 3-rw-r--r-- 1 root root 0 Jan 20 13:18 ruozedata.txt 像控制台输出打印：echo 12[root@hadoop614 ~]# echo Hello WorldHello World 输入输出重定向：&lt; &gt; &lt;&lt; &gt;&gt; 类型 符号 作用 标准输出重定向 命令 &gt; 文件 以覆盖的方式，把命令的正确输出输出到指定的文件或设备中 命令 &gt;&gt; 文件 以追加的方式，把命令的正确输出输出到指定的文件或设备中 标准错误输出重定向 错误命令2 &gt; 文件 以覆盖的方式，把命令的错误输出输出到指定的文件或设备中 错误命令2 &gt;&gt; 文件 以追加的方式，把命令的错误输出输出到指定的文件或设备中 正确输出和错误输出同时保存 命令 &gt; 文件2&gt;&amp;1 以捜盖的方式，把正确输出和错误输出都保存到同一个文件中 命令 &gt;&gt; 文件2&gt;&amp;1 以追加的方式，把正确输出和错误输出都保存到同一个文件中 命令&amp;&gt;文件 以覆盖的方式，把正确输出和错误输出都保存到同一个文件中 命令&amp;&gt;&gt;文件 以追加的方式，把正确输出和错误输出都保存到同一个文件中 命令&gt;&gt;文件1 2&gt;&gt;文件2 把正确的输出追加到文件1中，把错误的输出追加到文件2中 移动文件：mv # move的简写 mv + 原文件名/目录名 + 新名称 –以空格区分，可用于重命名移动的文件始终只有一份 12345678[root@hadoop614 ~]# mv ruozedata.txt ruozedata.txt_20190120[root@hadoop614 ~]# mv 1 direct[root@hadoop614 ~]# lltotal 12drwxr-xr-x 2 root root 4096 Jan 20 13:10 2drwxr-xr-x 2 root root 4096 Jan 20 13:10 3drwxr-xr-x 2 root root 4096 Jan 20 13:10 direct-rw-r--r-- 1 root root 0 Jan 20 13:12 ruozedata.txt_20190120 复制文件：cp #copy简写 cp + 原文件名/目录名 + 新名称 –以空格区分，可用于重命名负责的文件会有多份 1234[root@hadoop614 ~]# cp ruozedata.txt ./1/[root@hadoop614 ~]# ll ./1/total 0-rw-r--r-- 1 root root 0 Jan 20 13:23 ruozedata.txt 查看命令帮助：--help 123[root@hadoop614 ~]# ls --helpUsage: ls [OPTION]... [FILE]...····· # 内容过多，不做展示 查看文件内容：mroe/cat/tail 12345[root@hadoop614 ~]# cat ruozedata.txt # 直接向控制台打印文件所有内容a b cwww.ruozedata.comwww.baidu.com[root@hadoop614 ~]# 12345more 文件内容一页一页的往下翻 按空格键往下 回退不了 按q退出[root@hadoop614 etc]# more ld.so.cacheld.so-1.7.0 --More--(5%) 1less 文件内容 往下 往下 按上下箭头的按键 按q退出 别名：alias 1alias rzcd=&apos;cd /root/ruozedata/&apos; # 当前会话生效 配置环境变量 全局环境变量：/etc/profile #所有用户均可使用 12[root@hadoop614 ~]# vi /etc/profile # vim编译器[root@hadoop614 ~]# source /etc/profile # 使环境变量立即生效 个人环境变量：~/.base_profile # 只有所属用户生效12[root@hadoop614 ~]# vi ~/.bash_profile[root@hadoop614 ~]# source /etc/profile 删除：rm12[root@hadoop614 ~]# rm ruozedata.txt # 只能删除文件rm: remove regular file ‘ruozedata.txt’? y # 询问是否删除文件 y-&gt;yes;n-&gt;no 1234[root@hadoop614 ~]# rm -r 1 # -r 表示可以删除目录和文件rm: descend into directory ‘1’? y #1目录中有内容，是否进入1目录rm: remove regular empty file ‘1/ruozedata.txt’? y # 1目录中有文件，是否删除rm: remove directory ‘1’? y # 是否删除目录 123[root@hadoop614 ~]# rm -rf 2 # f参数表示强制，即不再询问是否进行下一步[root@hadoop614 ~]# 注：不允许使用使用 &apos;rm -rf /&apos; 命令 设置变量 变量：可以变化的量，以键值对的方式存在设置 key=value =前后不能有空格使用 ${key} 12345[root@hadoop614 ~]# user=xiaohai[root@hadoop614 ~]# echo $&#123;user&#125;xiaohai[root@hadoop614 ~]# echo $&#123;USER&#125; # shell 脚本中使用此方法获取当前登录的用户，或执行此脚本的用户root]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>运维</tag>
        <tag>Linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础命令一]]></title>
    <url>%2F2019%2F01%2F20%2FLinux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux 基础命令是作为运维及大数据开发人员必备技能之一 Linux系统用户12[root@hadoop614 ~]# iduid=0(root) gid=0(root) groups=0(root) # root为unix系统超级管理员用户 Linux基础命令 查看当前所在目录：pwd 12[root@hadoop614 ~]# pwd/root # /root为当前所在目录 切换所在目录：cd 1234567[root@hadoop614 ~]# cd /etc # / 表示根目录,为Linux系统一级目录 # etc为根目录下的二级目录 # ~为表示用户家目录 [root@hadoop614 etc]# # ‘#’表示用当前用户为管理员用户 # etc位置表示当前所在目录[root@hadoop614 etc]# cd # 切换到用户家目录[root@hadoop614 ~]# cd ~ # 切换到用户家目录 查看所在目录下内容：ls 12345678[root@hadoop614 ~]# ls # 查询当前目录下所有显性文件和目录名称1.txt 2.sh 3.ini 4.py [root@hadoop614 ~]# ls -l # -l :等同于‘ll’;显示当前目录内容及其属性 total 0 -rw-r--r-- 1 root root 0 Jan 20 12:34 1.txt # -rw-r--r-- 表示文件或文件夹权限 -rw-r--r-- 1 root root 0 Jan 20 12:34 2.sh # root表示文件或文件夹所属用户 -rw-r--r-- 1 root root 0 Jan 20 12:34 3.ini # root表示文件或文件夹所属组 -rw-r--r-- 1 root root 0 Jan 20 12:34 4.py # 后面依次为文件大小，更新月 日期 时间 名字 12345678910[root@hadoop614 ~]# ll -a # -a 表示查询当前目录下所有文件和目录名称(包含隐藏文件) # 隐藏文件/目录：以 . 开头的文件或目录total 48dr-xr-x---. 5 root root 4096 Jan 20 12:34 . #表示当前目录dr-xr-xr-x. 18 root root 4096 Jan 15 21:33 .. #表示父目录/上一级目录-rw------- 1 root root 321 Jan 16 23:20 .bash_history #用户历史输入命令记录文件-rw-r--r--. 1 root root 18 Dec 29 2013 .bash_logout # 用户登录记录文件-rw-r--r--. 1 root root 176 Dec 29 2013 .bash_profile # 用户环境变量配置文件-rw-r--r--. 1 root root 176 Dec 29 2013 .bashrc # 用户环境变量配置文件····· #内容过多，不做展示 123456[root@hadoop614 ~]# ll -rt # r 表示正序排序 t 表示时间 ：按时间正序排序total 8-rw-r--r-- 1 root root 0 Jan 20 12:34 4.py-rw-r--r-- 1 root root 0 Jan 20 12:34 3.ini-rw-r--r-- 1 root root 61 Jan 20 12:47 1.txt-rw-r--r-- 1 root root 2 Jan 20 12:48 2.sh 1234[root@hadoop614 ~]# ll -h #提升文件或目录大小的可读性total 660K-rw-r--r-- 1 root root 61 Jan 20 12:47 1.txt-rw-r--r-- 1 root root 655K Jan 20 12:50 services 清空当前屏幕：clear 查看当前系统网卡IP地址：ifconfig Linux : ifconfigWindows : ipconfigunix/AIX : ifconifg -a 123456789101112131415[root@hadoop614 ~]# ifconfigeth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.17.64.32 netmask 255.255.240.0 broadcast 172.17.79.255 ether 00:16:3e:0e:05:61 txqueuelen 1000 (Ethernet) RX packets 93473 bytes 8748270 (8.3 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 77688 bytes 8952210 (8.5 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 创建目录 ：mkdir123456[root@hadoop614 ~]# mkdir abc # 创建目录名为 abc[root@hadoop614 ~]# lltotal 664-rw-r--r-- 1 root root 61 Jan 20 12:47 1.txt # -rw-r--r-- 第一个‘-’表示文件drwxr-xr-x 2 root root 4096 Jan 20 13:03 abc # drwxr-xr-x 中的’d&apos;表示目录directory-rw-r--r-- 1 root root 670293 Jan 20 12:50 services 1234[root@hadoop614 ~]# mkdir -p a/b/c # -p：表示若创建目标目录的父目录不存在则创建父目录[root@hadoop614 ~]# ll a/btotal 4drwxr-xr-x 2 root root 4096 Jan 20 13:08 c 123456[root@hadoop614 ~]# mkdir 1 2 3 # 同时创建多个目录，目录名间以空格区分[root@hadoop614 ~]# lltotal 12drwxr-xr-x 2 root root 4096 Jan 20 13:10 1drwxr-xr-x 2 root root 4096 Jan 20 13:10 2drwxr-xr-x 2 root root 4096 Jan 20 13:10 3 创建文件：touch #创建空文件 touch + 文件名 1234567 [root@hadoop614 ~]# touch ruozedata.txt [root@hadoop614 ~]# lltotal 12drwxr-xr-x 2 root root 4096 Jan 20 13:10 1drwxr-xr-x 2 root root 4096 Jan 20 13:10 2drwxr-xr-x 2 root root 4096 Jan 20 13:10 3-rw-r--r-- 1 root root 0 Jan 20 13:18 ruozedata.txt 像控制台输出打印：echo 12[root@hadoop614 ~]# echo Hello WorldHello World 输入输出重定向：&lt; &gt; &lt;&lt; &gt;&gt; 类型 符号 作用 标准输出重定向 命令 &gt; 文件 以覆盖的方式，把命令的正确输出输出到指定的文件或设备中 命令 &gt;&gt; 文件 以追加的方式，把命令的正确输出输出到指定的文件或设备中 标准错误输出重定向 错误命令2 &gt; 文件 以覆盖的方式，把命令的错误输出输出到指定的文件或设备中 错误命令2 &gt;&gt; 文件 以追加的方式，把命令的错误输出输出到指定的文件或设备中 正确输出和错误输出同时保存 命令 &gt; 文件2&gt;&amp;1 以捜盖的方式，把正确输出和错误输出都保存到同一个文件中 命令 &gt;&gt; 文件2&gt;&amp;1 以追加的方式，把正确输出和错误输出都保存到同一个文件中 命令&amp;&gt;文件 以覆盖的方式，把正确输出和错误输出都保存到同一个文件中 命令&amp;&gt;&gt;文件 以追加的方式，把正确输出和错误输出都保存到同一个文件中 命令&gt;&gt;文件1 2&gt;&gt;文件2 把正确的输出追加到文件1中，把错误的输出追加到文件2中 移动文件：mv # move的简写 mv + 原文件名/目录名 + 新名称 –以空格区分，可用于重命名移动的文件始终只有一份 12345678[root@hadoop614 ~]# mv ruozedata.txt ruozedata.txt_20190120[root@hadoop614 ~]# mv 1 direct[root@hadoop614 ~]# lltotal 12drwxr-xr-x 2 root root 4096 Jan 20 13:10 2drwxr-xr-x 2 root root 4096 Jan 20 13:10 3drwxr-xr-x 2 root root 4096 Jan 20 13:10 direct-rw-r--r-- 1 root root 0 Jan 20 13:12 ruozedata.txt_20190120 复制文件：cp #copy简写 cp + 原文件名/目录名 + 新名称 –以空格区分，可用于重命名负责的文件会有多份 1234[root@hadoop614 ~]# cp ruozedata.txt ./1/[root@hadoop614 ~]# ll ./1/total 0-rw-r--r-- 1 root root 0 Jan 20 13:23 ruozedata.txt 查看命令帮助：--help 123[root@hadoop614 ~]# ls --helpUsage: ls [OPTION]... [FILE]...····· # 内容过多，不做展示 查看文件内容：mroe/cat/tail 12345[root@hadoop614 ~]# cat ruozedata.txt # 直接向控制台打印文件所有内容a b cwww.ruozedata.comwww.baidu.com[root@hadoop614 ~]# 12345more 文件内容一页一页的往下翻 按空格键往下 回退不了 按q退出[root@hadoop614 etc]# more ld.so.cacheld.so-1.7.0 --More--(5%) 1less 文件内容 往下 往下 按上下箭头的按键 按q退出 别名：alias 1alias rzcd=&apos;cd /root/ruozedata/&apos; # 当前会话生效 配置环境变量 全局环境变量：/etc/profile #所有用户均可使用 12[root@hadoop614 ~]# vi /etc/profile # vim编译器[root@hadoop614 ~]# source /etc/profile # 使环境变量立即生效 个人环境变量：~/.base_profile # 只有所属用户生效12[root@hadoop614 ~]# vi ~/.bash_profile[root@hadoop614 ~]# source /etc/profile 删除：rm12[root@hadoop614 ~]# rm ruozedata.txt # 只能删除文件rm: remove regular file ‘ruozedata.txt’? y # 询问是否删除文件 y-&gt;yes;n-&gt;no 1234[root@hadoop614 ~]# rm -r 1 # -r 表示可以删除目录和文件rm: descend into directory ‘1’? y #1目录中有内容，是否进入1目录rm: remove regular empty file ‘1/ruozedata.txt’? y # 1目录中有文件，是否删除rm: remove directory ‘1’? y # 是否删除目录 123[root@hadoop614 ~]# rm -rf 2 # f参数表示强制，即不再询问是否进行下一步[root@hadoop614 ~]# 注：不允许使用使用 &apos;rm -rf /&apos; 命令 设置变量 变量：可以变化的量，以键值对的方式存在设置 key=value =前后不能有空格使用 ${key} 12345[root@hadoop614 ~]# user=xiaohai[root@hadoop614 ~]# echo $&#123;user&#125;xiaohai[root@hadoop614 ~]# echo $&#123;USER&#125; # shell 脚本中使用此方法获取当前登录的用户，或执行此脚本的用户root]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>运维</tag>
        <tag>Linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xshell连接Linux系统]]></title>
    <url>%2F2019%2F01%2F17%2Fxshell%E8%BF%9E%E6%8E%A5Linux%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[日常工作，我们不会可能去机房登录Linux系统学习中我们同样不直接登录操作系统而是使用工具进行访问常用的工具有：xshell 和 CRT下面我提供了一个绿色破解版Xshell软件，也可以自己去官网或者应用商店下载 链接：https://pan.baidu.com/s/1JE-ndERbwKiFYnFDQJ4Uiw 提取码：0iaf 下载并解压搜，进入Xshell Plus目录双击运行”！绿化.bat” 即可 打开Xshell软件写入配置的名称，及linux的ip地址。如果是云主机，请配置公网地址。 点击确定，在页面中选择刚刚配置的名称后点击连接 输入用户名和密码 点击确定，如下图所示，说明已连接成功]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>基础</tag>
        <tag>Linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云环境准备]]></title>
    <url>%2F2019%2F01%2F17%2F%E9%98%BF%E9%87%8C%E4%BA%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[云主机请自行购买购买配置建议：2核4G 或以上 云主机设置1）修改实例名和HostName主机名:hadoop2）重置云主机root用户登录密码3）配置安全组规则配置出口22号端口4）访问连接方式：Xshell/CRT]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>基础</tag>
        <tag>Linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云环境准备]]></title>
    <url>%2F2019%2F01%2F17%2FVMware%E9%83%A8%E7%BD%B2Linux%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[使用软件及操作系统 软件：VMware10 操作系统： CentOS-6.5 下载链接：https://pan.baidu.com/s/1iCT3fVId7K9gQNosYf70xA 提取码：r2kz 开始安装Linux虚拟机安装VMware10 解压下载好的VMware10 进入解压后的VMware10目录，双击执行VMwareWorkstation_10.01_Lite_CHS.exe 等待释放完成 释放完成后，按步骤一次执行： 等待安装结束安装Linux虚拟机创建新的虚拟机 在桌面打开刚刚安装好的VMware workstation 10 点击创建新的虚拟机 在新建虚拟机向导中选择自定义(高级) 将操作系统镜像绑定到虚拟机 编辑虚拟机启动虚拟机,安装操作系统 在启动界面选择第一项 选择Skip进行跳过测试 将分配给虚拟机的存储进行格式化 时区选择上海 这是管理员密码 划分区 分区参考值 Mount Point File System Type Size(MB) Additional Size Options /boot ext4 200 Fixed size swap 2048 Fixed size / ext4 all free size Fill to maximum allowable size 格式化刚刚划分的文件系统 将配置写入磁盘 安装桌面版Linux系统 等待安装完成启动Linux进行初始设置 安装完成后，点击reboot进行重启 设置一个普通用户 使用root用户登录操作系统 修改主机名在系统桌面–&gt;鼠标右键–&gt;选择“Open in Terminal”进入命令行界面 1. 编辑 /etc/sysconfig/network123vi /etc/sysconfig/networkNETWORKING=yesHOSTNAME=hadoop001 2. hostname 直接命名12hostname=hadoopecho $hostname 3. 编辑/etc/hosts1echo &quot;IP地址 hadoop001&quot; &gt;&gt; /etc/hosts 配置静态IP并访问外网1、关闭Windows本地防火墙 打开控制面板–&gt;查看方式：小图标–&gt;Windows防火墙–&gt; 打开或关闭Windows防火墙–&gt; 关闭Windows防火墙2、关闭虚拟机防火墙 在Linux虚拟机中打开命令行，执行以下语句，关闭防护墙 1service iptables stop 设置关闭Linux防火墙开机自启动 1chkconfig iptables off 3、在Windows本地查看VM虚拟机的服务状态 同时按住：Win + r 建，打开运行窗口，输入services.msc 打开服务窗口 找到 VMware DHCP Service和VMware NAT Service服务 确定服务已启动，启动类型为自启动4、Windows本地在已连接的网卡上做网络共享 鼠标右键点击网络图标，选择打开网络和共享中心–&gt; 选择 本地连接5、使用ipconfig -all命令查看DNS、IPV4等信息6、配置VM8的选择使用配置IP7、进入VM10 ，点击菜单栏–&gt;编辑–&gt; 虚拟网络编辑器配置完成后–&gt;应用–&gt;确定8、设置虚拟机网络为NAT网络9、进入Linux虚拟机，编辑ifcfg-eth01vi /etc/sysconfig/network-scripts/ifcfg-eth0 重启网卡:service network restart10、 通过浏览器进行验证访问成功]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>基础</tag>
        <tag>Linux基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人生成长之开启大数据学习之旅]]></title>
    <url>%2F2019%2F01%2F17%2F%E5%BC%80%E5%90%AF%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85%2F</url>
    <content type="text"><![CDATA[开启在 若泽数据 的大数据学习之旅 如何学习大数据？ 搭建学习环境 调整学习心态 若泽数据阶段性学习目录1）Linux2）shell3）MySql4）SQL5）Hadoop6）Hive7）离线项目8）Python9）人工智能 目标通过在若泽数据几个月的学习阶段，尽快了解并掌握互联网行业及大数据方向的专业知识。提高个人能力水平]]></content>
      <tags>
        <tag>若泽数据</tag>
        <tag>大数据</tag>
        <tag>基础</tag>
      </tags>
  </entry>
</search>
