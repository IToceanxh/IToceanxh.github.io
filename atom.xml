<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小海的博客</title>
  <icon>https://www.gravatar.com/avatar/bd6bfd5b7ca385fec756b730fdd6adfa</icon>
  <subtitle>人生苦短，及时行乐</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://IToceanxh.github.io/"/>
  <updated>2019-05-02T07:24:53.197Z</updated>
  <id>http://IToceanxh.github.io/</id>
  
  <author>
    <name>Harm灬小海</name>
    <email>lhf_sum@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark入门</title>
    <link href="http://IToceanxh.github.io/2019/05/02/Spark%E5%85%A5%E9%97%A8/"/>
    <id>http://IToceanxh.github.io/2019/05/02/Spark入门/</id>
    <published>2019-05-02T06:49:41.000Z</published>
    <updated>2019-05-02T07:24:53.197Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h3><a id="more"></a><h3 id="为什么选择Spark"><a href="#为什么选择Spark" class="headerlink" title="为什么选择Spark"></a>为什么选择Spark</h3><h4 id="MapReduce的局限性"><a href="#MapReduce的局限性" class="headerlink" title="MapReduce的局限性"></a>MapReduce的局限性</h4><h5 id="繁杂"><a href="#繁杂" class="headerlink" title="繁杂"></a>繁杂</h5><ul><li>MapReduce提供了两种算子：map/reduce，不论是复杂的还是简单的，都需要map和reduce<br>当然，MapJoin中是不需要reduce阶段的<ol><li>low-level</li><li>constrained</li></ol></li><li>测试</li></ul><h5 id="效率低"><a href="#效率低" class="headerlink" title="效率低"></a>效率低</h5><ul><li>进程级别： MapTask和ReduceTask的进程过多</li><li>I/O： 网络/磁盘<br>map阶段输出的数据需要落到磁盘后，reduce阶段才能或取代到输入</li><li>排序： MapReduce执行流程中每个阶段都会进行排序，但是很多阶段的排序是没有必要进行的</li><li>Memory: 只可以做有限的基于内存的处理动作</li></ul><h5 id="不适合迭代处理"><a href="#不适合迭代处理" class="headerlink" title="不适合迭代处理"></a>不适合迭代处理</h5>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;什么是Spark&quot;&gt;&lt;a href=&quot;#什么是Spark&quot; class=&quot;headerlink&quot; title=&quot;什么是Spark&quot;&gt;&lt;/a&gt;什么是Spark&lt;/h3&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://IToceanxh.github.io/categories/Spark/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Scala基础数据类型</title>
    <link href="http://IToceanxh.github.io/2019/05/02/Scala%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>http://IToceanxh.github.io/2019/05/02/Scala基础数据类型/</id>
    <published>2019-05-02T06:42:52.000Z</published>
    <updated>2019-05-02T06:49:36.761Z</updated>
    
    <content type="html"><![CDATA[<h3 id="变量的定义和赋值"><a href="#变量的定义和赋值" class="headerlink" title="变量的定义和赋值"></a>变量的定义和赋值</h3><ul><li>scala中变量的定义和赋值有两种方法 <code>val</code> 和 <code>var</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val lname:String = &quot;xiaohai&quot;</span><br><span class="line">name: String = xiaohai</span><br><span class="line"></span><br><span class="line">scala&gt; val lage:Int = 19</span><br><span class="line">age: Int = 19</span><br><span class="line"></span><br><span class="line">scala&gt; var rname:String = &quot;xiaohai&quot;</span><br><span class="line">name: String = xiaohai</span><br><span class="line"></span><br><span class="line">scala&gt; var rage:Int = 19</span><br><span class="line">age: Int = 19</span><br></pre></td></tr></table></figure></li></ul><a id="more"></a><h3 id="scala基础数据数据类型"><a href="#scala基础数据数据类型" class="headerlink" title="scala基础数据数据类型"></a>scala基础数据数据类型</h3><table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody><tr><td>Int</td><td>数字</td></tr></tbody></table><hr><h2 id="持续更新ing…"><a href="#持续更新ing…" class="headerlink" title="持续更新ing…"></a>持续更新ing…</h2>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;变量的定义和赋值&quot;&gt;&lt;a href=&quot;#变量的定义和赋值&quot; class=&quot;headerlink&quot; title=&quot;变量的定义和赋值&quot;&gt;&lt;/a&gt;变量的定义和赋值&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;scala中变量的定义和赋值有两种方法 &lt;code&gt;val&lt;/code&gt; 和 &lt;code&gt;var&lt;/code&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;scala&amp;gt; val lname:String = &amp;quot;xiaohai&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;name: String = xiaohai&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;scala&amp;gt; val lage:Int = 19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;age: Int = 19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;scala&amp;gt; var rname:String = &amp;quot;xiaohai&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;name: String = xiaohai&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;scala&amp;gt; var rage:Int = 19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;age: Int = 19&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="开发" scheme="http://IToceanxh.github.io/categories/%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Scala" scheme="http://IToceanxh.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala入门仪式</title>
    <link href="http://IToceanxh.github.io/2019/05/02/Scala%E5%85%A5%E9%97%A8%E4%BB%AA%E5%BC%8F/"/>
    <id>http://IToceanxh.github.io/2019/05/02/Scala入门仪式/</id>
    <published>2019-05-02T05:47:15.000Z</published>
    <updated>2019-05-02T06:45:14.015Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>编程工具 IDEA<br>IDEA安装及破解步骤： <a href="https://blog.csdn.net/weixin_42330251/article/details/88714364" target="_blank" rel="noopener">点击跳转</a></p></li><li><p>Hello World<br>在Idea中新建Scala项目，在项目中新建HelloApp的object，代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.bigdata.scala01</span><br><span class="line"></span><br><span class="line">object HelloApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    println(&quot;Hello World&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;编程工具 IDEA&lt;br&gt;IDEA安装及破解步骤： &lt;a href=&quot;https://blog.csdn.net/weixin_42330251/article/details/88714364&quot; target=&quot;_blank&quot; rel=&quot;noopener
      
    
    </summary>
    
      <category term="开发" scheme="http://IToceanxh.github.io/categories/%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Scala" scheme="http://IToceanxh.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>企业级项目开发流程</title>
    <link href="http://IToceanxh.github.io/2019/04/06/%E4%BC%81%E4%B8%9A%E7%BA%A7%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B/"/>
    <id>http://IToceanxh.github.io/2019/04/06/企业级项目开发流程/</id>
    <published>2019-04-06T00:17:58.000Z</published>
    <updated>2019-05-02T04:39:39.210Z</updated>
    
    <content type="html"><![CDATA[<h3 id="项目调研"><a href="#项目调研" class="headerlink" title="项目调研"></a>项目调研</h3><ul><li>从业务角度出发对项目进行调研</li><li>人员：专业产品经理、非常熟悉业务的人员、项目经理等</li></ul><a id="more"></a><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><ul><li>需求分析阶段是不需要关系如何实现的，主要明确要做什么，要做成什么样子的</li><li>用户提出的显式的需求</li><li>项目中可能隐藏的需求 ： 一般由技术人员提出</li></ul><h3 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a>方案设计</h3><ul><li>概要设计 ： 明确功能点等</li><li>详细设计 ： 如何实现、使用那种架构、设计的技术、设计的数据库等信息，甚至包括伪代码等</li><li>系统设计 ： 设计系统是否可以扩展定制化等</li></ul><h3 id="功能开发"><a href="#功能开发" class="headerlink" title="功能开发"></a>功能开发</h3><ul><li>根据详细设计写代码</li><li>单元测试 ： CICD</li></ul><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><ul><li>功能测试</li><li>联调 ： 和其他团队进行联合测试</li><li>性能测试/压力测试</li><li>用户测试 ： 用户进行试用</li></ul><h3 id="部署上线"><a href="#部署上线" class="headerlink" title="部署上线"></a>部署上线</h3><ul><li>试运行阶段 ： 对比DIFF 和 稳定度</li><li>正式上线 ： 恢复发布</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;项目调研&quot;&gt;&lt;a href=&quot;#项目调研&quot; class=&quot;headerlink&quot; title=&quot;项目调研&quot;&gt;&lt;/a&gt;项目调研&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;从业务角度出发对项目进行调研&lt;/li&gt;
&lt;li&gt;人员：专业产品经理、非常熟悉业务的人员、项目经理等&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>基于Hadoop生态离线日志分析项目--分析</title>
    <link href="http://IToceanxh.github.io/2019/04/05/%E5%9F%BA%E4%BA%8EHadoop%E7%94%9F%E6%80%81%E7%A6%BB%E7%BA%BF%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE--%E5%88%86%E6%9E%90/"/>
    <id>http://IToceanxh.github.io/2019/04/05/基于Hadoop生态离线日志分析项目--分析/</id>
    <published>2019-04-05T12:39:37.000Z</published>
    <updated>2019-05-01T11:24:03.307Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul><li>开始进入若泽数据学习进阶阶段，在进阶阶段均以生产项目实战为出发点，首先来学习基于hadoop生态圈的离线日志分析</li></ul><a id="more"></a><h3 id="相关知识点"><a href="#相关知识点" class="headerlink" title="相关知识点"></a>相关知识点</h3><h4 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h4><ul><li>hdfs 存储</li><li>MapReduce 数据清洗</li><li>YARN</li><li>Hadoop集群的搭建</li></ul><h4 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h4><ul><li>使用外部表</li><li>SQL</li><li>解决数据倾斜</li><li>机遇元数据的管理</li><li>SQL如何转换为MapReduce</li><li>以及一些相关优化</li></ul><h4 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h4><ul><li>使用flume进行数据采集</li></ul><h4 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h4><ul><li>初期： crontab shell</li><li>后期： Azkaban</li></ul><h4 id="HUE：可视化的notebook"><a href="#HUE：可视化的notebook" class="headerlink" title="HUE：可视化的notebook"></a>HUE：可视化的notebook</h4><ul><li>进行数据排查</li></ul><h4 id="项目：通用的一些知识点"><a href="#项目：通用的一些知识点" class="headerlink" title="项目：通用的一些知识点"></a>项目：通用的一些知识点</h4><ul><li>开发流程</li><li>分工</li><li>应用场景</li><li>常用数据平台</li><li>集群规模及资源评估</li></ul><h3 id="整体开发流程"><a href="#整体开发流程" class="headerlink" title="整体开发流程"></a>整体开发流程</h3><ul><li>数据采集</li><li>ETL    <em>**</em></li><li>将数据数据分析数据 移动到 外部表分区目录    <em>**</em></li><li>业务统计：SQL ==&gt; dest table    <em>**</em></li><li>WebUi</li></ul><h3 id="通用离线架构"><a href="#通用离线架构" class="headerlink" title="通用离线架构"></a>通用离线架构</h3><p><img src="https://img-blog.csdnimg.cn/20190409203903499.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjMzMDI1MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;开始进入若泽数据学习进阶阶段，在进阶阶段均以生产项目实战为出发点，首先来学习基于hadoop生态圈的离线日志分析&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="进阶" scheme="http://IToceanxh.github.io/tags/%E8%BF%9B%E9%98%B6/"/>
    
  </entry>
  
  <entry>
    <title>重新定义hdfs的数据目录和日志目录</title>
    <link href="http://IToceanxh.github.io/2019/03/02/%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89hdfs%E7%9A%84%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95%E5%92%8C%E6%97%A5%E5%BF%97%E7%9B%AE%E5%BD%95/"/>
    <id>http://IToceanxh.github.io/2019/03/02/重新定义hdfs的数据目录和日志目录/</id>
    <published>2019-03-02T06:44:59.000Z</published>
    <updated>2019-05-01T09:32:25.031Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>hdfs默认的数据存放路径实在<code>/tmp/hadoop-${USER}/</code>目录,前面已经提到，Linux系统对/tmp目录存在定期清理机制，那么我们的数据目录存放在这里是很危险的。<br>希望将dfs目录和log目录都放到~/data目录中<br><a id="more"></a></p><h3 id="配置log日志路径"><a href="#配置log日志路径" class="headerlink" title="配置log日志路径"></a>配置log日志路径</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim $&#123;HADOOP_HOME&#125;/etc/hadoop/hadoop_env.sh</span><br><span class="line"># Where log files are stored.  $HADOOP_HOME/logs by default.</span><br><span class="line">export HADOOP_LOG_DIR=/home/hadoop/data/logs</span><br></pre></td></tr></table></figure><h3 id="修改dfs配置"><a href="#修改dfs配置" class="headerlink" title="修改dfs配置"></a>修改dfs配置</h3><ul><li>通过官网参数查询，发现都存放在<code>${hadoop.tmp.dir}</code>目录中,需要根据官方参数重新定义<code>${hadoop.tmp.dir}</code>目录</li></ul><h4 id="停止hdfs"><a href="#停止hdfs" class="headerlink" title="停止hdfs"></a>停止hdfs</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure><h4 id="修改name目录"><a href="#修改name目录" class="headerlink" title="修改name目录"></a>修改name目录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim etc/hadoop/hdfs-site.xml</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/home/hadoop/data/dfs/name&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br></pre></td></tr></table></figure><h4 id="修改data"><a href="#修改data" class="headerlink" title="修改data"></a>修改data</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/data/dfs/data&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h4 id="修改secondarynamenode"><a href="#修改secondarynamenode" class="headerlink" title="修改secondarynamenode"></a>修改secondarynamenode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/data/dfs/namesecondary&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h4 id="格式化namenode"><a href="#格式化namenode" class="headerlink" title="格式化namenode"></a>格式化namenode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure><h4 id="启动hdfs"><a href="#启动hdfs" class="headerlink" title="启动hdfs"></a>启动hdfs</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><h4 id="检查结果"><a href="#检查结果" class="headerlink" title="检查结果"></a>检查结果</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ll ~/data/dfs</span><br><span class="line">ll ~/data/logs</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;hdfs默认的数据存放路径实在&lt;code&gt;/tmp/hadoop-${USER}/&lt;/code&gt;目录,前面已经提到，Linux系统对/tmp目录存在定期清理机制，那么我们的数据目录存放在这里是很危险的。&lt;br&gt;希望将dfs目录和log目录都放到~/data目录中&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="基础" scheme="http://IToceanxh.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>修改hdfs和yarn的pid目录</title>
    <link href="http://IToceanxh.github.io/2019/03/02/%E4%BF%AE%E6%94%B9hdfs%E5%92%8Cyarn%E7%9A%84pid%E7%9B%AE%E5%BD%95/"/>
    <id>http://IToceanxh.github.io/2019/03/02/修改hdfs和yarn的pid目录/</id>
    <published>2019-03-02T05:20:09.000Z</published>
    <updated>2019-05-01T09:23:24.441Z</updated>
    
    <content type="html"><![CDATA[<p>Linux操作系统中会默认定期清理/tmp目录中的文件，而hdfs和yarn的pid文件默认存放在/tmp目录下，为避免pid文件被误删，修改默认的文件目录</p><a id="more"></a><h5 id="修改hdfs集群的pid文件位置"><a href="#修改hdfs集群的pid文件位置" class="headerlink" title="修改hdfs集群的pid文件位置"></a>修改hdfs集群的pid文件位置</h5><ul><li><p>配置HADOOP_PID_DIR</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ~/app/hadoop-2.6.0-cdh5.7.0/</span><br><span class="line">vi etc/hadoop/hadoop-env.sh </span><br><span class="line"># 修改HADOOP_PID_DIR的值</span><br><span class="line">export HADOOP_PID_DIR=~/data/tmp</span><br></pre></td></tr></table></figure></li><li><p>重启hadoop集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ ll ~/data/tmp</span><br><span class="line">total 12</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 6 Mar  2 12:39 hadoop-hadoop-datanode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 6 Mar  2 12:38 hadoop-hadoop-namenode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 6 Mar  2 12:39 hadoop-hadoop-secondarynamenode.pid</span><br></pre></td></tr></table></figure></li></ul><h5 id="修改yarn的pid目录"><a href="#修改yarn的pid目录" class="headerlink" title="修改yarn的pid目录"></a>修改yarn的pid目录</h5><ul><li><p>配置YARN_PID_DIR</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ vim etc/hadoop/yarn-env.sh </span><br><span class="line"># 新增</span><br><span class="line">export YARN_PID_DIR=/home/hadoop/data/tmp</span><br></pre></td></tr></table></figure></li><li><p>重启yarn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stop-yarn.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ ll ~/data/tmp</span><br><span class="line">总用量 20</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 6 4月  30 21:35 hadoop-hadoop-datanode.pid</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 6 4月  30 21:35 hadoop-hadoop-namenode.pid</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 6 4月  30 21:35 hadoop-hadoop-secondarynamenode.pid</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 6 4月  30 21:40 yarn-hadoop-nodemanager.pid</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 6 4月  30 21:40 yarn-hadoop-resourcemanager.pid</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Linux操作系统中会默认定期清理/tmp目录中的文件，而hdfs和yarn的pid文件默认存放在/tmp目录下，为避免pid文件被误删，修改默认的文件目录&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="基础" scheme="http://IToceanxh.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Yarn伪分布式部署</title>
    <link href="http://IToceanxh.github.io/2019/03/02/Yarn%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
    <id>http://IToceanxh.github.io/2019/03/02/Yarn伪分布式部署/</id>
    <published>2019-03-02T05:15:09.000Z</published>
    <updated>2019-05-01T09:23:59.286Z</updated>
    
    <content type="html"><![CDATA[<p>目标：部署yarn伪分布式，并使MapReduce作业跑在yarn之上</p><blockquote><p>MapReduce 是用来做计算的 是jar包提交到yarn上 本身不需要部署<br>Yarn：资源和作业调度，需要单独部署</p></blockquote><a id="more"></a><h5 id="配置yarn和MapReduce"><a href="#配置yarn和MapReduce" class="headerlink" title="配置yarn和MapReduce"></a>配置yarn和MapReduce</h5><ul><li><p>配置mapred-site.xml<br>指定MapReduce调度器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line">cd app/hadoop-2.6.0-cdh5.7.0</span><br><span class="line">cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml</span><br><span class="line">vi etc/hadoop/mapred-site.xml</span><br><span class="line"># 添加</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>配置yarn-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vi etc/hadoop/yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>启动yarn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ sbin/start-yarn.sh </span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop614.out</span><br><span class="line">hadoop614: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop614.out</span><br></pre></td></tr></table></figure></li></ul><p>我们发现yarn启动了两个进程：<br>resourcemanager    #是一个资源管理者<br>nodemanager    # 是一个节点管理者<br>同时yarn提供了一个资源调度的UI页面：<a href="http://hadoop614:8088/cluster" target="_blank" rel="noopener">http://hadoop614:8088/cluster</a></p><h5 id="测试MapReduce"><a href="#测试MapReduce" class="headerlink" title="测试MapReduce"></a>测试MapReduce</h5><ul><li>计算圆周率<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 5 10</span><br></pre></td></tr></table></figure></li></ul><p>可以在yarn的UI页面查看作业详情。</p><ul><li>计算单词出现的次数（词频统计）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">## 创建a.log</span><br><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ vi a.log</span><br><span class="line">ruoze</span><br><span class="line">jepson</span><br><span class="line">www.ruozedata.com</span><br><span class="line">dashu</span><br><span class="line">adai</span><br><span class="line">fanren</span><br><span class="line">1</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">c</span><br><span class="line">a b c ruoze jepon</span><br><span class="line">## 创建b.log</span><br><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ vi b.log</span><br><span class="line">a b d e f ruoze</span><br><span class="line">1 1 3 5</span><br><span class="line">## 创建目录</span><br><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -mkdir -p /wordcount/input</span><br><span class="line"># 上传文件至/wordcount/input</span><br><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -put a.log /wordcount/input</span><br><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -put b.log /wordcount/input</span><br><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -ls /wordcount/input/</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         75 2019-04-30 21:16 /wordcount/input/a.log</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         24 2019-04-30 21:17 /wordcount/input/b.log</span><br><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /wordcount/input /wordcount/output</span><br><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -ls /wordcount/output</span><br><span class="line">-rw-r--r--   1 hadoop supergroup          0 2019-04-30 21:19 /wordcount/output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 hadoop supergroup        107 2019-04-30 21:19 /wordcount/output/part-r-00000</span><br><span class="line">[hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hadoop dfs -text /wordcount/output/part*</span><br><span class="line">13</span><br><span class="line">31</span><br><span class="line">51</span><br><span class="line">a3</span><br><span class="line">adai1</span><br><span class="line">b3</span><br><span class="line">c2</span><br><span class="line">d1</span><br><span class="line">dashu1</span><br><span class="line">e1</span><br><span class="line">f1</span><br><span class="line">fanren1</span><br><span class="line">jepon1</span><br><span class="line">jepson1</span><br><span class="line">ruoze3</span><br><span class="line">www.ruozedata.com1</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目标：部署yarn伪分布式，并使MapReduce作业跑在yarn之上&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MapReduce 是用来做计算的 是jar包提交到yarn上 本身不需要部署&lt;br&gt;Yarn：资源和作业调度，需要单独部署&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="基础" scheme="http://IToceanxh.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>解读jps命令和hadoop pid文件</title>
    <link href="http://IToceanxh.github.io/2019/03/02/%E8%A7%A3%E8%AF%BBjps%E5%91%BD%E4%BB%A4%E5%92%8Chadoop-pid%E6%96%87%E4%BB%B6/"/>
    <id>http://IToceanxh.github.io/2019/03/02/解读jps命令和hadoop-pid文件/</id>
    <published>2019-03-02T04:41:22.000Z</published>
    <updated>2019-05-01T09:23:41.280Z</updated>
    
    <content type="html"><![CDATA[<ul><li>jps命令的真相<br>首先我们先看jps命令的位置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ which jps</span><br><span class="line">/usr/java/jdk1.8.0_45/bin/jps</span><br></pre></td></tr></table></figure></li></ul><p>我们发现jps命令实在java目录下，说明jps是java JDK中的一个命令；<br>jps查看的时当前用户运行的java进程<br><a id="more"></a></p><ul><li><p>对应进程的表示文件位置<br>哪个用户起的java进程，其对应的进程表示文件在<code>/tmp/hsperfdata_${USER}</code>目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop614 ~]$ cd /tmp/hsperfdata_$&#123;USER&#125;</span><br><span class="line">[hadoop@hadoop614 hsperfdata_hadoop]$ ll</span><br><span class="line">总用量 192</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:28 27874</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:29 27994</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:29 28235</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:29 28451</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:29 28576</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:28 28632</span><br><span class="line">[hadoop@hadoop614 hsperfdata_hadoop]$ jps</span><br><span class="line">28576 NodeManager</span><br><span class="line">27874 NameNode</span><br><span class="line">29154 Jps</span><br><span class="line">28451 ResourceManager</span><br><span class="line">28632 RunJar</span><br><span class="line">27994 DataNode</span><br><span class="line">28235 SecondaryNameNode</span><br></pre></td></tr></table></figure></li><li><p>那么root作为超级用户，权限最高的用户，理论上root应该可以查看所有用户下的进程，以root执行 jps 命令的结果是什么呢</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# jps</span><br><span class="line">29248 Jps</span><br><span class="line">28576 -- process information unavailable</span><br><span class="line">27874 -- process information unavailable</span><br><span class="line">28451 -- process information unavailable</span><br><span class="line">28632 -- process information unavailable</span><br><span class="line">27994 -- process information unavailable</span><br><span class="line">28235 -- process information unavailable</span><br></pre></td></tr></table></figure></li></ul><p>我们可以看到使用root用户执行结果的进程数和pid与hadoop用户执行结果一致，但是进程名称却显示为<code>process information unavailable</code>,是不用状态；<br>因此，当我门需要以root用户来判断进程是否正常时，需要以<code>ps -ef</code>返回结果为准，以hadoop进程为例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# ps -ef | grep namenode &amp;&amp; grep dataname</span><br><span class="line">hadoop   27874     1  4 20:28 ?        00:00:24 /usr/java/jdk1.8.0_45/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs -Dhadoop.log.file=hadoop-hadoop-namenode-hadoop614.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode</span><br><span class="line">hadoop   28235     1  2 20:28 ?        00:00:16 /usr/java/jdk1.8.0_45/bin/java -Dproc_secondarynamenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs -Dhadoop.log.file=hadoop-hadoop-secondarynamenode-hadoop614.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode</span><br></pre></td></tr></table></figure></p><ul><li>当我们以root权限kill hadoop进程后可能发生的现象<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# kill -9 28576</span><br><span class="line">[root@hadoop614 ~]# ps -ef | grep 28576</span><br><span class="line">root     29407 29228  0 20:41 pts/1    00:00:00 grep 28576</span><br><span class="line">[root@hadoop614 ~]# jps</span><br><span class="line">29408 Jps</span><br><span class="line">28576 -- process information unavailable</span><br><span class="line">27874 -- process information unavailable</span><br><span class="line">28451 -- process information unavailable</span><br><span class="line">28632 -- process information unavailable</span><br><span class="line">27994 -- process information unavailable</span><br><span class="line">28235 -- process information unavailable</span><br></pre></td></tr></table></figure></li></ul><p>我们发现使用root用户强制kill hadoop进程后，进程已经不存在，但是jps命令依然可以查到<br>那么看下jps命令pid存放路径<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# ll /tmp/hsperfdata_hadoop</span><br><span class="line">总用量 192</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:42 27874</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:42 27994</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:42 28235</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:42 28451</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:40 28576</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:42 28632</span><br></pre></td></tr></table></figure></p><p>发现28576进程的文件依然存在，尝试删除此文件；<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# rm -f /tmp/hsperfdata_hadoop/28576 </span><br><span class="line">[root@hadoop614 ~]# ll /tmp/hsperfdata_hadoop</span><br><span class="line">总用量 160</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:44 27874</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:44 27994</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:44 28235</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:44 28451</span><br><span class="line">-rw-------. 1 hadoop hadoop 32768 4月  30 20:44 28632</span><br><span class="line">[root@hadoop614 ~]# jps</span><br><span class="line">27874 -- process information unavailable</span><br><span class="line">28451 -- process information unavailable</span><br><span class="line">29464 Jps</span><br><span class="line">28632 -- process information unavailable</span><br><span class="line">27994 -- process information unavailable</span><br><span class="line">28235 -- process information unavailable</span><br></pre></td></tr></table></figure></p><p>发现jps中进程已经不存在，说明使用root用户kill掉jps进程时，还需要删除相应目录下的pid文件，否则jps查询时次pid仍然存在，而hadoop启动和停止时会校验这个文件，核能到时hadoop集群启动失败</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;jps命令的真相&lt;br&gt;首先我们先看jps命令的位置&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ which jps&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/usr/java/jdk1.8.0_45/bin/jps&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们发现jps命令实在java目录下，说明jps是java JDK中的一个命令；&lt;br&gt;jps查看的时当前用户运行的java进程&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="基础" scheme="http://IToceanxh.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>以hostname启动hadoop伪分布式</title>
    <link href="http://IToceanxh.github.io/2019/03/02/%E4%BB%A5hostname%E5%90%AF%E5%8A%A8hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    <id>http://IToceanxh.github.io/2019/03/02/以hostname启动hadoop伪分布式/</id>
    <published>2019-03-02T03:22:51.000Z</published>
    <updated>2019-05-01T09:23:16.942Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><strong>搭建hdfs伪分部署</strong> ： <a href="https://itoceanxh.github.io/2019/03/02/%E6%90%AD%E5%BB%BAhdfs%E4%BC%AA%E5%88%86%E9%83%A8%E7%BD%B2/">点击跳转</a></p></li><li><p>通过《搭建hdfs伪分部署》，在启动阶段发现：namenode、namenode、secondarynamenode是以localhost和0.0.0.0启动的；<br>本片文章要实现的是以hostname来启动进程;<br>因为在日常生产中均以hostname来部署启动，具体方便在哪里，后面体会吧、<br><code>su - hadoop</code></p></li></ul><a id="more"></a><ul><li><p>配置core-site.xml文件，修改namenode进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi $HADOOP_HOME/etc/hadoop/core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop614:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>配置slaves文件，修改datanode进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi $HADOOP_HOME/etc/hadoop/slaves</span><br><span class="line">hadoop614</span><br></pre></td></tr></table></figure></li><li><p>配置 hdfs-site.xml 文件 修改 secondarynamenode 进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vi $HADOOP_HOME/etc/hadoop/hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop614:50090&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop614:50091&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>启动hdfs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">## 打印内容中可看到，三个进程均是以hadoop614启动，hadoop614是我的hostname</span><br><span class="line">19/03/02 11:19:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Starting namenodes on [hadoop614]</span><br><span class="line">hadoop614: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop614.out</span><br><span class="line">hadoop614: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop614.out</span><br><span class="line">Starting secondary namenodes [hadoop614]</span><br><span class="line">hadoop614: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop614.out</span><br><span class="line">19/03/02 11:19:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;搭建hdfs伪分部署&lt;/strong&gt; ： &lt;a href=&quot;https://itoceanxh.github.io/2019/03/02/%E6%90%AD%E5%BB%BAhdfs%E4%BC%AA%E5%88%86%E9%83%A8%E7%BD%B2/&quot;&gt;点击跳转&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;通过《搭建hdfs伪分部署》，在启动阶段发现：namenode、namenode、secondarynamenode是以localhost和0.0.0.0启动的；&lt;br&gt;本片文章要实现的是以hostname来启动进程;&lt;br&gt;因为在日常生产中均以hostname来部署启动，具体方便在哪里，后面体会吧、&lt;br&gt;&lt;code&gt;su - hadoop&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="基础" scheme="http://IToceanxh.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>搭建hdfs伪分部署</title>
    <link href="http://IToceanxh.github.io/2019/03/02/%E6%90%AD%E5%BB%BAhdfs%E4%BC%AA%E5%88%86%E9%83%A8%E7%BD%B2/"/>
    <id>http://IToceanxh.github.io/2019/03/02/搭建hdfs伪分部署/</id>
    <published>2019-03-02T02:35:35.000Z</published>
    <updated>2019-05-01T09:23:45.725Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>软件版本</strong></li></ul><table><thead><tr><th>软件</th><th>Hadoop</th><th>JDK</th></tr></thead><tbody><tr><td>版本</td><td>2.6.0-cdh5.7.0</td><td>jdk1.8.0_45</td></tr></tbody></table><p><strong>部署JDK飞机票：</strong><a href="https://itoceanxh.github.io/2019/03/02/Linux%E9%83%A8%E7%BD%B2jdk/">点击跳转</a><br><strong>配置ssh无密码访问localhost：</strong><a href="https://itoceanxh.github.io/2019/03/02/%E9%85%8D%E7%BD%AEssh%E6%97%A0%E5%AF%86%E7%A0%81%E8%AE%BF%E9%97%AElocalhost/">点击跳转</a></p><a id="more"></a><p>1.创建hadoop用户，管理所有hadoop生态软件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 创建用户</span><br><span class="line">user add</span><br><span class="line"># 切换到hadoop用户</span><br><span class="line">su - hadoop</span><br></pre></td></tr></table></figure></p><p>2.创建目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir app# hadoop组件安装目录</span><br><span class="line">mkdir software# hadoop组件安装包下载路径</span><br><span class="line">mkdir data# hadoop组件数据存放路径</span><br><span class="line">mkdir source # hadoop组件源码路径</span><br></pre></td></tr></table></figure></p><p>3.下载hadoop-2.6.0-cdh5.7.0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd software</span><br><span class="line">wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz</span><br></pre></td></tr></table></figure></p><p>4.解压hadoop-2.6.0-cdh5.7.0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ../app/</span><br></pre></td></tr></table></figure></p><p>5.配置etc/hadoop/hadoop-env.sh脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ../app/</span><br><span class="line">vim etc/hadoop/hadoop-env.sh</span><br><span class="line"># The java implementation to use.</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export HADOOP_PREFIC=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0</span><br></pre></td></tr></table></figure></p><p>6.编辑配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim etc/hadoop/core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim etc/hadoop/hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>7.配置环境变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo &apos;&apos;&apos;</span><br><span class="line">export HADOOP_PREFIX=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0</span><br><span class="line">export HADOOP_HOME=$&#123;HADOOP_PREFIX&#125;</span><br><span class="line">export PATH=$&#123;HADOOP_PREFIX&#125;/bin:PATH=$&#123;HADOOP_PREFIX&#125;/sbin:$PATH</span><br><span class="line">&apos;&apos;&apos; &gt;&gt; ~/.bash_profile</span><br></pre></td></tr></table></figure></p><p>8.启动hdfs<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cd ~/app/hadoop-2.6.0-cdh5.7.0</span><br><span class="line">bin/hdfs namenode -format</span><br><span class="line"># 打印类似以下内容</span><br><span class="line">19/03/02 10:22:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Starting namenodes on [localhost]</span><br><span class="line">localhost: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop614.out</span><br><span class="line">localhost: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop614.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br><span class="line">The authenticity of host &apos;0.0.0.0 (0.0.0.0)&apos; can&apos;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:FHC9m3uE6QV24WscKKQIPROVQv0RpFOrpOYl8xjJNpE.</span><br><span class="line">ECDSA key fingerprint is MD5:ec:90:80:a1:85:3f:3b:f2:d7:d9:0c:79:48:ce:56:f1.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">0.0.0.0: Warning: Permanently added &apos;0.0.0.0&apos; (ECDSA) to the list of known hosts.</span><br><span class="line">0.0.0.0: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop614.out</span><br><span class="line">19/03/02 10:22:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br></pre></td></tr></table></figure></p><p>9.查看是否启动成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br><span class="line"># 打印</span><br><span class="line">10978 NameNode</span><br><span class="line">11368 Jps</span><br><span class="line">11259 SecondaryNameNode</span><br><span class="line">11101 DataNode</span><br></pre></td></tr></table></figure></p><p>10.使用hdfs命令帮助<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">hdfs</span><br><span class="line"># 打印</span><br><span class="line">    Usage: hdfs [--config confdir] COMMAND</span><br><span class="line">          where COMMAND is one of:</span><br><span class="line">     dfs                  run a filesystem command on the file systems supported in Hadoop.</span><br><span class="line">     namenode -format     format the DFS filesystem</span><br><span class="line">     secondarynamenode    run the DFS secondary namenode</span><br><span class="line">     namenode             run the DFS namenode</span><br><span class="line">     journalnode          run the DFS journalnode</span><br><span class="line">     zkfc                 run the ZK Failover Controller daemon</span><br><span class="line">     datanode             run a DFS datanode</span><br><span class="line">     dfsadmin             run a DFS admin client</span><br><span class="line">     haadmin              run a DFS HA admin client</span><br><span class="line">     fsck                 run a DFS filesystem checking utility</span><br><span class="line">     balancer             run a cluster balancing utility</span><br><span class="line">     jmxget               get JMX exported values from NameNode or DataNode.</span><br><span class="line">     mover                run a utility to move block replicas across</span><br><span class="line">                          storage types</span><br><span class="line">     oiv                  apply the offline fsimage viewer to an fsimage</span><br><span class="line">     oiv_legacy           apply the offline fsimage viewer to an legacy fsimage</span><br><span class="line">     oev                  apply the offline edits viewer to an edits file</span><br><span class="line">     fetchdt              fetch a delegation token from the NameNode</span><br><span class="line">     getconf              get config values from configuration</span><br><span class="line">     groups               get the groups which users belong to</span><br><span class="line">     snapshotDiff         diff two snapshots of a directory or diff the</span><br><span class="line">                          current directory contents with a snapshot</span><br><span class="line">     lsSnapshottableDir   list all snapshottable dirs owned by the current user</span><br><span class="line">                        Use -help to see options</span><br><span class="line">     portmap              run a portmap service</span><br><span class="line">     nfs3                 run an NFS version 3 gateway</span><br><span class="line">     cacheadmin           configure the HDFS cache</span><br><span class="line">     crypto               configure HDFS encryption zones</span><br><span class="line">     storagepolicies      list/get/set block storage policies</span><br><span class="line">     version              print the version</span><br><span class="line"></span><br><span class="line">    Most commands print help when invoked w/o parameters.</span><br><span class="line"></span><br><span class="line">    [hadoop@hadoop614 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs</span><br><span class="line">    Usage: hadoop fs [generic options]</span><br><span class="line">       [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">       [-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">       [-checksum &lt;src&gt; ...]</span><br><span class="line">       [-chgrp [-R] GROUP PATH...]</span><br><span class="line">       [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">       [-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">       [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">       [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">       [-count [-q] [-h] [-v] &lt;path&gt; ...]</span><br><span class="line">       [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">       [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">       [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">       [-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">       [-du [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">       [-expunge]</span><br><span class="line">       [-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">       [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">       [-getfacl [-R] &lt;path&gt;]</span><br><span class="line">       [-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">       [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">       [-help [cmd ...]]</span><br><span class="line">       [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">       [-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">       [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">       [-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">       [-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">       [-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">       [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">       [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">       [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">       [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">       [-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">       [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">       [-stat [format] &lt;path&gt; ...]</span><br><span class="line">       [-tail [-f] &lt;file&gt;]</span><br><span class="line">       [-test -[defsz] &lt;path&gt;]</span><br><span class="line">       [-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">       [-touchz &lt;path&gt; ...]</span><br><span class="line">       [-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">    Generic options supported are</span><br><span class="line">    -conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">    -D &lt;property=value&gt;            use value for given property</span><br><span class="line">    -fs &lt;local|namenode:port&gt;      specify a namenode</span><br><span class="line">    -jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">    -files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">    -libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include in the classpath.</span><br><span class="line">    -archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">    The general command line syntax is</span><br><span class="line">    bin/hadoop command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;strong&gt;软件版本&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;软件&lt;/th&gt;
&lt;th&gt;Hadoop&lt;/th&gt;
&lt;th&gt;JDK&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;版本&lt;/td&gt;
&lt;td&gt;2.6.0-cdh5.7.0&lt;/td&gt;
&lt;td&gt;jdk1.8.0_45&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;部署JDK飞机票：&lt;/strong&gt;&lt;a href=&quot;https://itoceanxh.github.io/2019/03/02/Linux%E9%83%A8%E7%BD%B2jdk/&quot;&gt;点击跳转&lt;/a&gt;&lt;br&gt;&lt;strong&gt;配置ssh无密码访问localhost：&lt;/strong&gt;&lt;a href=&quot;https://itoceanxh.github.io/2019/03/02/%E9%85%8D%E7%BD%AEssh%E6%97%A0%E5%AF%86%E7%A0%81%E8%AE%BF%E9%97%AElocalhost/&quot;&gt;点击跳转&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="基础" scheme="http://IToceanxh.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop入门篇</title>
    <link href="http://IToceanxh.github.io/2019/03/02/Hadoop%E5%85%A5%E9%97%A8%E7%AF%87/"/>
    <id>http://IToceanxh.github.io/2019/03/02/Hadoop入门篇/</id>
    <published>2019-03-02T02:35:20.000Z</published>
    <updated>2019-05-01T09:25:20.435Z</updated>
    
    <content type="html"><![CDATA[<p>问：什么是Hadoop？<br>答：</p><ol><li>从广义上来讲，是以Apache Hadoop软件为中心的生态圈，包括但不限于：hive、zookeeper、spark、hbase等</li><li>从狭义上讲，就是 Apache Hadoop软件</li><li>Hadoop 是Apache基金会下一个开源的分布式计算平台，它以分布式文件系统HDFS和MapReduce算法为核心，为用户提供了系统底层细节透明的分布式基础架构。</li></ol><a id="more"></a><h4 id="hadoop的优点"><a href="#hadoop的优点" class="headerlink" title="hadoop的优点"></a>hadoop的优点</h4><ul><li>优点：Hadoop是一个开源框架，可编写和运行分布式应用来处理大规模数据，分布式计算是一个不断变化且宽泛的领域，优点如下：<h5 id="易用性"><a href="#易用性" class="headerlink" title="易用性"></a>易用性</h5>Hadoop运行在由一般商用机器构成的大型集群上。<h5 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h5>Hadoop致力于一般商用机器上，其架构假设硬件会频繁出现失效，它可以从容处理大多数此类故障。<h5 id="可扩展"><a href="#可扩展" class="headerlink" title="可扩展"></a>可扩展</h5>Hadoop通过增加集群节点，可以线性地拓展以处理更大数据集。<h5 id="简单"><a href="#简单" class="headerlink" title="简单"></a>简单</h5>Hadoop允许用户快速的编写出高效地并行代码。</li></ul><h4 id="比较Hadoop和SQL数据库"><a href="#比较Hadoop和SQL数据库" class="headerlink" title="比较Hadoop和SQL数据库"></a>比较Hadoop和SQL数据库</h4><ul><li>从总体上看，现在大多数数据应用处理的主力是关系型数据库，即SQL面向的是结构化的数据，而Hadoop则针对的是非结构化的数据，从这一角度看，Hadoop提供了对数据处理的一种更为通用的方式。<h5 id="详细比较"><a href="#详细比较" class="headerlink" title="详细比较"></a>详细比较</h5>下面，我们从特定的视角将Hadoop与SQL数据库做详细比较：<h6 id="用scale-out代替scale-up"><a href="#用scale-out代替scale-up" class="headerlink" title="用scale-out代替scale-up"></a>用scale-out代替scale-up</h6>拓展商用服务器的代价是非常昂贵的。要运行一个更大的数据库，就要一个更大的服务器，事实上，各服务器厂商往往会把其昂贵的高端机标称为“数据库级服务器”，不过有时候有可能需要处理更大的数据集，但却找不到更大的机器，而更为重要的是，高端机对于许多应用并不经济。<h6 id="用键值对代替关系表"><a href="#用键值对代替关系表" class="headerlink" title="用键值对代替关系表"></a>用键值对代替关系表</h6>关系型数据库需要将数据按照某种模式存放到具有关系型数据结构表中，但是许多当前的数据模型并不能很好的适应这些模型，如文本、图片、xml等，此外，大型数据集往往是非结构化或半结构化的。而Hadoop以键值对作为最基本的数据单元，能够灵活的处理较少结构化的数据类型。<h6 id="用函数式编程（MapReduce）代替声明式查询（SQL）"><a href="#用函数式编程（MapReduce）代替声明式查询（SQL）" class="headerlink" title="用函数式编程（MapReduce）代替声明式查询（SQL）"></a>用函数式编程（MapReduce）代替声明式查询（SQL）</h6>SQL从根本上说是一个高级声明式语言，它的手段是声明你想要的结果，并让数据库引擎判断如何获取数据。而在MapReduce程序中，实际的数据处理步骤是由你指定的。SQL使用查询语句，而MapReduce使用程序和脚本。MapReduce还可以建立复杂的数据统计模型，或者改变图像数据的处理格式。<h6 id="用离线批量处理代替在线处理"><a href="#用离线批量处理代替在线处理" class="headerlink" title="用离线批量处理代替在线处理"></a>用离线批量处理代替在线处理</h6>Hadoop并不适合处理那种对几条记录读写的在线事务处理模式，而适合一次写入多次读取的数据需求。</li></ul><p><span>博客内容采摘自：Heaven Wang 的博客：<a href="https://www.cnblogs.com/heavenwang/p/3988033.html" target="_blank" rel="noopener">Hadoop详解一：Hadoop简介</a></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;问：什么是Hadoop？&lt;br&gt;答：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从广义上来讲，是以Apache Hadoop软件为中心的生态圈，包括但不限于：hive、zookeeper、spark、hbase等&lt;/li&gt;
&lt;li&gt;从狭义上讲，就是 Apache Hadoop软件&lt;/li&gt;
&lt;li&gt;Hadoop 是Apache基金会下一个开源的分布式计算平台，它以分布式文件系统HDFS和MapReduce算法为核心，为用户提供了系统底层细节透明的分布式基础架构。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="基础" scheme="http://IToceanxh.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Linux部署jdk</title>
    <link href="http://IToceanxh.github.io/2019/03/02/Linux%E9%83%A8%E7%BD%B2jdk/"/>
    <id>http://IToceanxh.github.io/2019/03/02/Linux部署jdk/</id>
    <published>2019-03-02T02:35:10.000Z</published>
    <updated>2019-05-01T09:25:20.441Z</updated>
    
    <content type="html"><![CDATA[<p>在官网下载JDK1.8版本并上传到/opt目录</p><ol><li>创建目录<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/java</span><br><span class="line">mkdir /user/share/java</span><br></pre></td></tr></table></figure></li></ol><a id="more"></a><ol start="2"><li><p>解压JDK安装包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /opt/jdk-8u45-linux-x64.gz -C /usr/java</span><br></pre></td></tr></table></figure></li><li><p>修改解压后JDK目录权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R root:root /usr/java/jdk1.8.0_45/</span><br></pre></td></tr></table></figure></li><li><p>配置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;&quot;&quot;</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export JRE_HOME=$JAVA_HOME/jre</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH</span><br><span class="line">export PATH=$JAVA_HOME/bin:$JRE_HOME/lib:$PATH</span><br><span class="line">&quot;&quot;&quot; &gt;&gt; /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>刷新环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>验证部署是否成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">which java</span><br><span class="line">java -version</span><br><span class="line">echo $JAVA_HOME</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在官网下载JDK1.8版本并上传到/opt目录&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建目录&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;mkdir /usr/java&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mkdir /user/share/java&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="运维" scheme="http://IToceanxh.github.io/categories/%E8%BF%90%E7%BB%B4/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="基础" scheme="http://IToceanxh.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="运维" scheme="http://IToceanxh.github.io/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>配置ssh无密码访问localhost</title>
    <link href="http://IToceanxh.github.io/2019/03/02/%E9%85%8D%E7%BD%AEssh%E6%97%A0%E5%AF%86%E7%A0%81%E8%AE%BF%E9%97%AElocalhost/"/>
    <id>http://IToceanxh.github.io/2019/03/02/配置ssh无密码访问localhost/</id>
    <published>2019-03-02T02:35:00.000Z</published>
    <updated>2019-05-01T09:23:31.075Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>删除默认的.ssh目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line">rm -rf ~/.ssh</span><br></pre></td></tr></table></figure></li><li><p>生成新的ssh配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssk-keygen# 按四次回车</span><br></pre></td></tr></table></figure></li><li><p>配置信任关系</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></li><li><p>修改信任文件权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></li><li><p>验证</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost date# 首次连接需要输入yes，不需输入密码即配置成功</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;删除默认的.ssh目录&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;l
      
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="基础" scheme="http://IToceanxh.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>MySQL基础一</title>
    <link href="http://IToceanxh.github.io/2019/01/28/MySQL%E5%9F%BA%E7%A1%80%E4%B8%80/"/>
    <id>http://IToceanxh.github.io/2019/01/28/MySQL基础一/</id>
    <published>2019-01-28T13:38:46.000Z</published>
    <updated>2019-05-01T09:24:21.151Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL默认用户及密码修改<br>MySQL数据库的集中登录方式及参数</p><a id="more"></a><h6 id="使用空用户登录"><a href="#使用空用户登录" class="headerlink" title="使用空用户登录"></a>使用空用户登录</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# su - mysqladmin</span><br><span class="line">Last login: Sun Jan 27 18:22:49 CST 2019 on pts/0</span><br><span class="line">[mysqladmin@hadoop614 ~]$ mysql -h127.0.0.1</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 8</span><br><span class="line">Server version: 5.6.23-log MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; exit</span><br><span class="line">Bye</span><br></pre></td></tr></table></figure><h6 id="使用空用户登录-1"><a href="#使用空用户登录-1" class="headerlink" title="使用空用户登录"></a>使用空用户登录</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[mysqladmin@hadoop614 ~]$ mysql -uroot -p -h127.0.0.1</span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 9</span><br><span class="line">Server version: 5.6.23-log MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; exit</span><br><span class="line">Bye</span><br></pre></td></tr></table></figure><h6 id="删除空用户，设置root密码"><a href="#删除空用户，设置root密码" class="headerlink" title="删除空用户，设置root密码"></a>删除空用户，设置root密码</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">## 查询数据库</span><br><span class="line">mysql&gt; show databases;</span><br><span class="line">## 切换数据库</span><br><span class="line">mysql&gt; use mysql</span><br><span class="line">## 查询数据库表</span><br><span class="line">mysql&gt; show tables;</span><br><span class="line">## 查询表数据</span><br><span class="line">mysql&gt; select user,password,host from user；</span><br><span class="line">+------+----------+-----------+</span><br><span class="line">| user | password | host      |</span><br><span class="line">+------+----------+-----------+</span><br><span class="line">| root |          | localhost |</span><br><span class="line">| root |          | hadoop614 |</span><br><span class="line">| root |          | 127.0.0.1 |</span><br><span class="line">| root |          | ::1       |</span><br><span class="line">|      |          | localhost |</span><br><span class="line">|      |          | hadoop614 |</span><br><span class="line">+------+----------+-----------+</span><br><span class="line">6 rows in set (0.00 sec)</span><br><span class="line">## 删除空用户</span><br><span class="line">mysql&gt; delete from user where user = &apos;&apos;;</span><br><span class="line">Query OK, 2 rows affected (0.00 sec)</span><br><span class="line">## 更新root用户密码</span><br><span class="line">mysql&gt; update user set password=password(123456) where user=&apos;root&apos;;</span><br><span class="line">Query OK, 4 rows affected (0.00 sec)</span><br><span class="line">Rows matched: 4  Changed: 4  Warnings: 0</span><br><span class="line">## 查询表数据</span><br><span class="line">mysql&gt; select user,password,host from user；</span><br><span class="line">+------+-------------------------------------------+-----------+</span><br><span class="line">| user | password                                  | host      |</span><br><span class="line">+------+-------------------------------------------+-----------+</span><br><span class="line">| root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | localhost |</span><br><span class="line">| root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | hadoop614 |</span><br><span class="line">| root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | 127.0.0.1 |</span><br><span class="line">| root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | ::1       |</span><br><span class="line">+------+-------------------------------------------+-----------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><blockquote><p>#针对用户 权限的操作语句 养成习惯 都最后一步执行刷新权限<br>flush privileges;</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; exit</span><br><span class="line">Bye</span><br></pre></td></tr></table></figure><h6 id="重新登录数据库"><a href="#重新登录数据库" class="headerlink" title="重新登录数据库"></a>重新登录数据库</h6><p>不输入密码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mysqladmin@hadoop614 ~]$ mysql -uroot -p -h127.0.0.1</span><br><span class="line">Enter password: </span><br><span class="line">ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: NO)</span><br></pre></td></tr></table></figure></p><p>正常输入密码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[mysqladmin@hadoop614 ~]$ mysql -uroot -p -h127.0.0.1</span><br><span class="line">Enter password: </span><br><span class="line">ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)</span><br><span class="line">[mysqladmin@hadoop614 ~]$ mysql -uroot -p -h127.0.0.1</span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 16</span><br><span class="line">Server version: 5.6.23-log MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></p><h6 id="添加用户MySql环境变量"><a href="#添加用户MySql环境变量" class="headerlink" title="添加用户MySql环境变量"></a>添加用户MySql环境变量</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[mysqladmin@hadoop614 ~]$ vi .bash_profile</span><br><span class="line"># .bash_profile</span><br><span class="line"></span><br><span class="line"># Get the aliases and functions</span><br><span class="line">if [ -f ~/.bashrc ]; then</span><br><span class="line">        . ~/.bashrc</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># User specific environment and startup programs</span><br><span class="line"></span><br><span class="line">PATH=$PATH:$HOME/.local/bin:$HOME/bin</span><br><span class="line">         </span><br><span class="line">MYSQL_HOME=/usr/local/mysql</span><br><span class="line">PATH=$&#123;MYSQL_HOME&#125;/bin:$&#123;PATH&#125;   </span><br><span class="line">export PATH</span><br><span class="line">PS1=`uname -n`&quot;:&quot;&apos;$USER&apos;&quot;:&quot;&apos;$PWD&apos;&quot;:&gt;&quot;; export PS1</span><br><span class="line">:wq!</span><br></pre></td></tr></table></figure><p>刷新环境变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mysqladmin@hadoop614 ~]$ source .bash_profile</span><br><span class="line">hadoop614:mysqladmin:/usr/local/mysql:&gt;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MySQL默认用户及密码修改&lt;br&gt;MySQL数据库的集中登录方式及参数&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="运维" scheme="http://IToceanxh.github.io/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="MySQL" scheme="http://IToceanxh.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>部署MySQL数据库</title>
    <link href="http://IToceanxh.github.io/2019/01/28/%E9%83%A8%E7%BD%B2MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://IToceanxh.github.io/2019/01/28/部署MySQL数据库/</id>
    <published>2019-01-28T13:38:45.000Z</published>
    <updated>2019-05-01T09:23:49.497Z</updated>
    
    <content type="html"><![CDATA[<p>部署MySQL数据库的两种方式</p><a id="more"></a><p>一、使用yum方式安装数据库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install mysql</span><br></pre></td></tr></table></figure></p><p>二、 自定义安装</p><ol><li>准备MySQL安装包，上传到linux</li><li><p>查询是否安装过MySQL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 local]# ps -ef|grep mysqld | grep -v grep</span><br><span class="line">[root@hadoop614 local]# rpm -qa |grep -i mysql</span><br><span class="line">[root@hadoop614 local]#</span><br></pre></td></tr></table></figure></li><li><p>解压压缩文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 local]# tar zvfx mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz </span><br><span class="line">[root@hadoop614 local]# mv mysql-5.6.23-linux-glibc2.5-x86_64 mysql</span><br><span class="line">[root@hadoop614 local]# ll -d mysql*</span><br><span class="line">drwxr-xr-x 13 root root      4096 Jan 27 17:10 mysql</span><br><span class="line">-rw-r--r--  1 root root 311771412 Jan 27 16:18 mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>创建用户</p><ul><li><p>检查是否存在dba组和mysqladmin用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 local]# cat /etc/group | grep dba</span><br><span class="line">[root@hadoop614 local]# cat /etc/passwd | grep mysqladmin</span><br><span class="line">[root@hadoop614 local]#</span><br></pre></td></tr></table></figure></li><li><p>创建用户组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 local]# groupadd -g 101 dba# -g 制定组id</span><br><span class="line">[root@hadoop614 local]# cat /etc/group | grep dba</span><br><span class="line">dba:x:101:</span><br></pre></td></tr></table></figure></li><li><p>创建用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 local]# useradd -u 501 -g dba -G root -d /usr/local/mysql mysqladmin</span><br><span class="line">useradd: warning: the home directory already exists.# 提示用户家目录已经存在</span><br><span class="line">Not copying any file from skel directory into it.# 没有cp skel到家目录中</span><br><span class="line">[root@hadoop614 etc]# id mysqladmin</span><br><span class="line">uid=501(mysqladmin) gid=101(dba) groups=101(dba),0(root)</span><br></pre></td></tr></table></figure></li><li><p>复制配置文件到mysqladmin家目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 skel]# cp /etc/skel/.* /usr/local/mysql</span><br><span class="line">cp: omitting directory ‘.’</span><br><span class="line">cp: omitting directory ‘..’</span><br></pre></td></tr></table></figure></li></ul></li><li><p>配置my.cnf配置文件</p></li></ol><blockquote><p>程序查找配置文件顺序</p><p>#defualt start: /etc/my.cnf –&gt;/etc/mysql/my.cnf – &gt;SYSCONFDIR/my.cnf<br>-&gt;$MYSQL_HOME/my.cnf-&gt; –defaults-extra-file-&gt;~/my.cnf </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# cd /etc/</span><br><span class="line">[root@hadoop614 etc]# vi my.cnf</span><br><span class="line">[client]</span><br><span class="line">port            = 3306</span><br><span class="line">socket          = /usr/local/mysql/data/mysql.sock</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">port            = 3306</span><br><span class="line">socket          = /usr/local/mysql/data/mysql.sock</span><br><span class="line"></span><br><span class="line">skip-external-locking</span><br><span class="line">key_buffer_size = 256M</span><br><span class="line">sort_buffer_size = 2M</span><br><span class="line">read_buffer_size = 2M</span><br><span class="line">read_rnd_buffer_size = 4M</span><br><span class="line">query_cache_size= 32M</span><br><span class="line">max_allowed_packet = 16M</span><br><span class="line">myisam_sort_buffer_size=128M</span><br><span class="line">tmp_table_size=32M</span><br><span class="line"></span><br><span class="line">table_open_cache = 512</span><br><span class="line">thread_cache_size = 8</span><br><span class="line">wait_timeout = 86400</span><br><span class="line">interactive_timeout = 86400</span><br><span class="line">max_connections = 600</span><br><span class="line"></span><br><span class="line"># Try number of CPU&apos;s*2 for thread_concurrency</span><br><span class="line">thread_concurrency = 32</span><br><span class="line"></span><br><span class="line">#isolation level and default engine </span><br><span class="line">default-storage-engine = INNODB</span><br><span class="line">transaction-isolation = READ-COMMITTED</span><br><span class="line"></span><br><span class="line">server-id  = 1</span><br><span class="line">basedir     = /usr/local/mysql</span><br><span class="line">datadir     = /usr/local/mysql/data</span><br><span class="line">pid-file     = /usr/local/mysql/data/hostname.pid</span><br><span class="line"></span><br><span class="line">#open performance schema</span><br><span class="line">log-warnings</span><br><span class="line">sysdate-is-now</span><br><span class="line"></span><br><span class="line">binlog_format = MIXED</span><br><span class="line">log_bin_trust_function_creators=1</span><br><span class="line">log-error  = /usr/local/mysql/data/hostname.err</span><br><span class="line">log-bin=/usr/local/mysql/arch/mysql-bin</span><br><span class="line">#other logs</span><br><span class="line">#general_log =1</span><br><span class="line">#general_log_file  = /usr/local/mysql/data/general_log.err</span><br><span class="line">#slow_query_log=1</span><br><span class="line">#slow_query_log_file=/usr/local/mysql/data/slow_log.err</span><br><span class="line"></span><br><span class="line">#for replication slave</span><br><span class="line">#log-slave-updates </span><br><span class="line">#sync_binlog = 1</span><br><span class="line"></span><br><span class="line">#for innodb options </span><br><span class="line">innodb_data_home_dir = /usr/local/mysql/data/</span><br><span class="line">innodb_data_file_path = ibdata1:500M:autoextend</span><br><span class="line">innodb_log_group_home_dir = /usr/local/mysql/arch</span><br><span class="line">innodb_log_files_in_group = 2</span><br><span class="line">innodb_log_file_size = 200M</span><br><span class="line"></span><br><span class="line">#生产上 机械硬盘 sata盘 5000r 7200 10000 15000 ==&gt; ssd 生产</span><br><span class="line"># innodb_buffer_pool_size 调大 8G</span><br><span class="line"></span><br><span class="line">innodb_buffer_pool_size = 1024M </span><br><span class="line">innodb_additional_mem_pool_size = 50M</span><br><span class="line">innodb_log_buffer_size = 16M</span><br><span class="line"></span><br><span class="line">innodb_lock_wait_timeout = 100</span><br><span class="line">#innodb_thread_concurrency = 0</span><br><span class="line">innodb_flush_log_at_trx_commit = 1</span><br><span class="line">innodb_locks_unsafe_for_binlog=1</span><br><span class="line"></span><br><span class="line">#innodb io features: add for mysql5.5.8</span><br><span class="line">performance_schema</span><br><span class="line">innodb_read_io_threads=4</span><br><span class="line">innodb-write-io-threads=4</span><br><span class="line">innodb-io-capacity=200</span><br><span class="line">#purge threads change default(0) to 1 for purge</span><br><span class="line">innodb_purge_threads=1</span><br><span class="line">innodb_use_native_aio=on</span><br><span class="line"></span><br><span class="line">#case-sensitive file names and separate tablespace</span><br><span class="line">innodb_file_per_table = 1</span><br><span class="line">lower_case_table_names=1</span><br><span class="line"></span><br><span class="line">[mysqldump]</span><br><span class="line">quick</span><br><span class="line">max_allowed_packet = 16M</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">no-auto-rehash</span><br><span class="line"></span><br><span class="line">[mysqlhotcopy]</span><br><span class="line">interactive-timeout</span><br><span class="line"></span><br><span class="line">[myisamchk]</span><br><span class="line">key_buffer_size = 256M</span><br><span class="line">sort_buffer_size = 256M</span><br><span class="line">read_buffer = 2M</span><br><span class="line">write_buffer = 2M</span><br></pre></td></tr></table></figure><p>6.修改my.cnf配置文件权限和所属<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 etc]# chmod 640 my.cnf</span><br><span class="line">[root@hadoop614 etc]# chown mysqladmin:dba my.cnf</span><br><span class="line">[root@hadoop614 etc]# ll my.cnf</span><br><span class="line">-rw-r----- 1 mysqladmin dba 2326 Jan 27 17:38 my.cnf</span><br></pre></td></tr></table></figure></p><p>7.修改sqladmin家目录权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 etc]# chmod -R 755 /usr/local/mysql</span><br><span class="line">[root@hadoop614 etc]# chown -R mysqladmin:dba /usr/local/mysql</span><br><span class="line">[root@hadoop614 etc]# ll -d /usr/local/mysql</span><br><span class="line">drwxr-xr-x 13 mysqladmin dba 4096 Jan 27 17:31 /usr/local/mysql</span><br></pre></td></tr></table></figure></p><p>8.创建arch目录 存储binlog 归档日志</p><blockquote><p>实时同步 MySQL数据到hbase<br>mysql–maxwell–kafka–ss–hbase</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 etc]# su - mysqladmin</span><br><span class="line">[mysqladmin@hadoop614 ~]$ pwd</span><br><span class="line">/usr/local/mysql</span><br><span class="line">[mysqladmin@hadoop614 ~]$ ll -d arch/</span><br><span class="line">drwxr-xr-x 2 mysqladmin dba 4096 Jan 27 17:46 arch/</span><br></pre></td></tr></table></figure><p>9.安装MySQL数据库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 etc]# su - mysqladmin</span><br><span class="line">[mysqladmin@hadoop614 ~]$ pwd</span><br><span class="line">/usr/local/mysql</span><br><span class="line">[mysqladmin@hadoop614 ~]$ scripts/mysql_install_db  \</span><br><span class="line">&gt; --user=mysqladmin \</span><br><span class="line">&gt; --basedir=/usr/local/mysql \</span><br><span class="line">&gt; --datadir=/usr/local/mysql/data</span><br></pre></td></tr></table></figure></p><p>10.配置MySQL服务并引导自动启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[mysqladmin@hadoop614 support-files]$ exit</span><br><span class="line">logout</span><br><span class="line"># 将服务文件拷贝到init.d下，并重命名为mysql</span><br><span class="line">[root@hadoop614 ~]# cd /usr/local/mysql</span><br><span class="line">[root@hadoop614 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql </span><br><span class="line">#赋予可执行权限</span><br><span class="line">[root@hadoop614 mysql]# chmod +x /etc/rc.d/init.d/mysql</span><br><span class="line">#删除服务</span><br><span class="line">[root@hadoop614 mysql]# chkconfig --del mysql</span><br><span class="line">[#添加服务</span><br><span class="line">root@hadoop614 mysql]# chkconfig --add mysql</span><br><span class="line">[root@hadoop614 mysql]# chkconfig --level 345 mysql on</span><br><span class="line">[root@hadoop614 mysql]# vi /etc/rc.local</span><br><span class="line">在文件中追加：su - mysqladmin -c &quot;/etc/init.d/mysql start --federated&quot;</span><br></pre></td></tr></table></figure></p><p>11.启动MySQL并查看进程和侦听<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 mysql]# su - mysqladmin</span><br><span class="line">Last login: Sun Jan 27 18:04:31 CST 2019 on pts/1</span><br><span class="line">[mysqladmin@hadoop614 ~]$  rm -rf my.cnf</span><br><span class="line">方法一：</span><br><span class="line">[mysqladmin@hadoop614 ~]$ service mysql start</span><br><span class="line">Starting MySQL..[  OK  ]</span><br><span class="line">[mysqladmin@hadoop614 ~]$ service mysql status</span><br><span class="line">MySQL running (14023)[  OK  ]</span><br><span class="line">方法二：</span><br><span class="line">[mysqladmin@hadoop614 ~]$ service mysql stop</span><br><span class="line">Shutting down MySQL..[  OK  ]</span><br><span class="line">[mysqladmin@hadoop614 ~]$ mysqld_safe &amp;</span><br><span class="line">[1] 14901</span><br><span class="line">[mysqladmin@hadoop614 ~]$ 190127 18:24:18 mysqld_safe Logging to &apos;/usr/local/mysql/data/hostname.err&apos;.</span><br><span class="line">190127 18:24:18 mysqld_safe Starting mysqld daemon with databases from /usr/local/mysql/data</span><br><span class="line"></span><br><span class="line">[mysqladmin@hadoop614 ~]$ service mysql status</span><br><span class="line">MySQL running (15544)[  OK  ]</span><br><span class="line">[mysqladmin@hadoop614 ~]$</span><br></pre></td></tr></table></figure></p><p>到这里，MySQL数据库已部署完成。</p><blockquote><p>部署过程中MySQL默认创建两个用户<br>一：root用户，密码为空<br>二：空用户，空密码</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;部署MySQL数据库的两种方式&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Linux基础" scheme="http://IToceanxh.github.io/tags/Linux%E5%9F%BA%E7%A1%80/"/>
    
      <category term="运维" scheme="http://IToceanxh.github.io/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="数据库" scheme="http://IToceanxh.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Linux基础命令三</title>
    <link href="http://IToceanxh.github.io/2019/01/26/Linux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E4%B8%89/"/>
    <id>http://IToceanxh.github.io/2019/01/26/Linux基础命令三/</id>
    <published>2019-01-26T10:20:48.000Z</published>
    <updated>2019-05-01T09:24:29.580Z</updated>
    
    <content type="html"><![CDATA[<p>学习Linux基础命令第三天</p><a id="more"></a><ul><li><h4 id="文本编辑器：vi"><a href="#文本编辑器：vi" class="headerlink" title="文本编辑器：vi"></a>文本编辑器：<code>vi</code></h4><blockquote><p>注：在生产环境/操作系统上直接编辑文本文件时，一定要先进行备份，再对源文件进行编辑修改操作。<br>例：需要修改用户环境变量时：</p></blockquote></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# cp .bash_profile .bash_profile_20190124</span><br></pre></td></tr></table></figure><h6 id="编辑器常用命令"><a href="#编辑器常用命令" class="headerlink" title="## 编辑器常用命令"></a>## 编辑器常用命令</h6><p>跳转到第一行首字符：<code>gg</code><br>跳转到左后一行首字符：<code>G</code><br>跳转到行尾：<code>shift $</code><br>跳转到行首：<code>shift ^</code><br>删除当前行：<code>dd</code><br>删除光标以下的所有行：<code>dG</code><br>删除光标一下n行：<code>ndd</code></p><h6 id="场景一：清楚文件内所有内容"><a href="#场景一：清楚文件内所有内容" class="headerlink" title="场景一：清楚文件内所有内容"></a>场景一：清楚文件内所有内容</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">方法一:</span><br><span class="line">[root@hadoop614 ~]# ll test.txt </span><br><span class="line">-rw-r--r-- 1 root root 42 Jan 26 15:08 test.txt#查看文件大小 42 字节</span><br><span class="line">[root@hadoop614 ~]# echo &gt; test.txt # 像文件内重定向一个空值替换文件所有内容</span><br><span class="line">[root@hadoop614 ~]# ll test.txt </span><br><span class="line">-rw-r--r-- 1 root root 1 Jan 26 15:09 test.txt# 查看文件大小为 1字节</span><br><span class="line">结论：文件大小还有1字节，判断时为非空文件！因echo输出本身自带换行符</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">方法二:</span><br><span class="line">[root@hadoop614 ~]# ll test.txt </span><br><span class="line">-rw-r--r-- 1 root root 42 Jan 26 15:08 test.txt#查看文件大小 42 字节</span><br><span class="line">[root@hadoop614 ~]# cat /dev/null &gt; test.txt# 将/dev/null内容输出到3.txt文件内，/dev/null为纯空</span><br><span class="line">[root@hadoop614 ~]# ll test.txt </span><br><span class="line">-rw-r--r-- 1 root root 0 Jan 26 15:19 test.txt</span><br><span class="line">结论：文件大小为0字节，判断时为空文件！因cat /dev/null返回无任何内容，无换行符或者空格等</span><br></pre></td></tr></table></figure><h6 id="场景二：在文件最后追加内容"><a href="#场景二：在文件最后追加内容" class="headerlink" title="场景二：在文件最后追加内容"></a>场景二：在文件最后追加内容</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# vi test.txt </span><br><span class="line">光标切换到最后一行：`G`</span><br><span class="line">光标切换到最后一个字符：`$`</span><br><span class="line">方法一：</span><br><span class="line">进入编辑模式：`i`#当前光标位置进行编辑</span><br><span class="line">光标像后移动一位：`→`</span><br><span class="line">回车进入下一行</span><br><span class="line">方法二：</span><br><span class="line">进入编辑模式：`a`# 当前光标位置后一位进行编辑</span><br><span class="line">回车进入下一行</span><br><span class="line">方法三：</span><br><span class="line">进入编辑模式：`o`# 当前光标位置下一行新建一行进行编辑</span><br></pre></td></tr></table></figure><h6 id="场景三：快速定位关键字-error"><a href="#场景三：快速定位关键字-error" class="headerlink" title="场景三：快速定位关键字 - error"></a>场景三：快速定位关键字 - error</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">方法一：</span><br><span class="line">[root@hadoop614 ~]# vi test.txt </span><br><span class="line">在尾行模式输入/error，回车即可向下查找</span><br><span class="line">或者光标在文件任意位置，可输入?error，回车即可向上查找</span><br><span class="line">注：确为当前光标位置：</span><br><span class="line">尾行模式输入 `:set nu`  可显示行号</span><br><span class="line">尾行模式输入 `:set nonu`  可取消显示行号</span><br><span class="line">方法二：</span><br><span class="line"># 可一次性输出文件内所有存在关键字的所有行</span><br><span class="line">[root@hadoop614 ~]# cat test.txt | grep error</span><br><span class="line">error 1 2 3</span><br><span class="line">error 4</span><br><span class="line">error</span><br><span class="line">方法三：</span><br><span class="line">将test.txt文件下载到本地，使用PadNotes++等文本编辑工具打开并查找</span><br></pre></td></tr></table></figure><h6 id="更多vi编辑器基础知识"><a href="#更多vi编辑器基础知识" class="headerlink" title="更多vi编辑器基础知识"></a>更多vi编辑器基础知识</h6><p><a href="https://blog.csdn.net/weixin_42330251/article/details/86613400" target="_blank" rel="noopener">【Linux基础】VIM编辑器常用命令详解（基础篇）</a></p><h3 id="权限控制"><a href="#权限控制" class="headerlink" title="权限控制"></a>权限控制</h3><h5 id="权限解读"><a href="#权限解读" class="headerlink" title="权限解读"></a>权限解读</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# su - hadoop</span><br><span class="line">[hadoop@hadoop614 ~]$ ll /root</span><br><span class="line">ls: cannot open directory /root: Permission denied# Permission denied  表示权限不足或者没有权限</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 切换回root用户，看看为什么hadoop用户无权访问/root</span><br><span class="line">[root@hadoop614 ~]# ll -d /root</span><br><span class="line">dr-xr-x---. 6 root root 4096 Jan 26 15:42 /root</span><br><span class="line"></span><br><span class="line"># ll命令查询出的结果介绍：</span><br><span class="line">/root# 文件或目录名</span><br><span class="line">Jan 26 15:42 # 表示文件/目录创建时间或文件更新时间</span><br><span class="line">4096 # 表示文件大小和目录本身的大小，一般情况下目录统一为4096，因为这个值不包含目录下的文件和子目录内文件的大小</span><br><span class="line">root root# 第一个root表示文件或目录所属用户--所属者</span><br><span class="line"># 第二个root表示文件或目录所属组--所属组</span><br><span class="line">dr-xr-x---# 第1位：d:表示目录-:表示文件l:表示软硬链接(软链接类似于快捷方式)</span><br><span class="line"># 第2-4位：r-x：表示目录或文件所属者的权限</span><br><span class="line"># 第5-7位：r-x：表示目录或文件所属组成员用户的权限</span><br><span class="line"># 第8-10位：---：表示其他用户对目录或文件的权限</span><br></pre></td></tr></table></figure><blockquote><p>权限说明：<br>    -：表示无权限 – 0<br>x：表示执行权限 – 1<br>w：表示可写权限 – 2<br>r：表示刻度权限 –4<br>解读/root目录权限：550<br>dr-xr-x—. 6 root root 4096 Jan 26 15:42 /root<br>所属者：root    拥有权限：读+执行 = 4+1 = 5<br>所属组：root    拥有权限：读+执行 = 4+1 = 5<br>其他用户拥有权限：无 = 0</p></blockquote><h5 id="权限修改"><a href="#权限修改" class="headerlink" title="权限修改"></a>权限修改</h5><p>修改文件或目录权限：<code>chmod</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# ll test.txt</span><br><span class="line">-rw-r--r-- 1 root root 295 Jan 26 15:42 test.txt# test.txt文件权限为644</span><br><span class="line">[root@hadoop614 ~]# chmod 755 test.txt</span><br><span class="line">[root@hadoop614 ~]# ll test.txt </span><br><span class="line">-rwxr-xr-x 1 root root 295 Jan 26 15:42 test.txt# test.txt文件权限为755</span><br></pre></td></tr></table></figure></p><blockquote><p>若要一起修改目录及其子文件和子目录的权限，则使用<code>chmod -R</code>参数</p></blockquote><p>修改文件或目录所属者和所属组：<code>chown</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# chown hadoop:hadoop001 test.txt </span><br><span class="line">[root@hadoop614 ~]# ll test.txt</span><br><span class="line">-rwxr-xr-x 1 hadoop hadoop001 295 Jan 26 15:42 test.txt# 此时文件所属以被修改为hadoop:hadoop001(所属者:所属组)</span><br></pre></td></tr></table></figure></p><blockquote><p>若要一起修改目录及其子文件和子目录的所属，则使用<code>chown -R</code>参数</p></blockquote><ul><li><h4 id="执行shell脚本"><a href="#执行shell脚本" class="headerlink" title="执行shell脚本"></a>执行shell脚本</h4><blockquote><p>shell脚本也是文本文件，一般命名后缀为<code>.sh</code></p></blockquote></li></ul><p>想要文件可执行，首先需要赋予文件可执行权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# ll bigdata.sh </span><br><span class="line">-rw-r--r-- 1 root root 66 Jan 26 16:34 bigdata.sh# 文件权限644 没有执行权限</span><br></pre></td></tr></table></figure></p><blockquote><p>执行脚本的两种方法<br>方法一：<code>./bigdata.sh</code><br>方法二：<code>sh bigdata.sh</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# ./bigdata.sh</span><br><span class="line">-bash: ./bigdata.sh: Permission denied# 没有执行权限是会返回权限不足</span><br><span class="line">[root@hadoop614 ~]# chmod u+x bigdata.sh# 给所属者添加执行权限</span><br><span class="line">[root@hadoop614 ~]# ll bigdata.sh </span><br><span class="line">-rwxr--r-- 1 root root 23 Jan 26 16:49 bigdata.sh</span><br><span class="line">[root@hadoop614 ~]# ./bigdata.sh </span><br><span class="line">若泽大数据</span><br></pre></td></tr></table></figure></p></blockquote><ul><li><h5 id="软链接"><a href="#软链接" class="headerlink" title="软链接"></a>软链接</h5><blockquote><p>软连接：相当于Windows系统中的快捷方式，删除软连接而源文件删除，但通过软连接对文件进行编辑后，源文件也会修改</p></blockquote></li></ul><p>创建软连接：<code>ln -s</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# ln -s bigdata.sh bat</span><br><span class="line">[root@hadoop614 ~]# ll</span><br><span class="line">total 4</span><br><span class="line">lrwxrwxrwx 1 root root 10 Jan 26 16:56 bat -&gt; bigdata.sh</span><br><span class="line">-rwxr--r-- 1 root root 36 Jan 26 16:51 bigdata.sh</span><br><span class="line">[root@hadoop614 ~]# cat bat</span><br><span class="line">#!/bin/bash </span><br><span class="line">echo &quot;若泽大数据&quot;</span><br></pre></td></tr></table></figure></p><h5 id="远程上传-下载：rz-sz"><a href="#远程上传-下载：rz-sz" class="headerlink" title="远程上传/下载：rz/sz"></a>远程上传/下载：<code>rz/sz</code></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 安装传输工具：</span><br><span class="line">[root@hadoop614 ~]# yum -y install lrzsz</span><br></pre></td></tr></table></figure><blockquote><h6 id="上传文件：rz"><a href="#上传文件：rz" class="headerlink" title="上传文件：rz"></a>上传文件：<code>rz</code></h6><p>选择需要上传的文件 –&gt; 点击打开<img src="https://img-blog.csdnimg.cn/20190126170722923.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjMzMDI1MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上传完成<img src="https://img-blog.csdnimg.cn/20190126171000719.png" alt="在这里插入图片描述"></p></blockquote><blockquote><h6 id="下载文件：sz"><a href="#下载文件：sz" class="headerlink" title="下载文件：sz"></a>下载文件：<code>sz</code></h6><p><code>[root@hadoop614 ~]# sz bigdata.sh</code><br><img src="https://img-blog.csdnimg.cn/20190126171342424.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjMzMDI1MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>bat文件下载完成<br><img src="https://img-blog.csdnimg.cn/20190126171609376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjMzMDI1MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></blockquote><ul><li><h4 id="系统命令"><a href="#系统命令" class="headerlink" title="系统命令"></a>系统命令</h4><h5 id="系统资源使用了监控：top"><a href="#系统资源使用了监控：top" class="headerlink" title="系统资源使用了监控：top"></a>系统资源使用了监控：<code>top</code></h5><blockquote><p>可监控系统资源使用率情况<br>通常情况下，生产环境<code>load average: 0.00, 0.01, 0.05</code>经验值不可超过10<br>&lt;<a href="https://blog.csdn.net/weixin_42330251/article/details/86659144" target="_blank" rel="noopener">【Linux基础】top命令的用法详细详解</a>&gt;</p></blockquote></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">top - 17:19:46 up 5 days, 10:10,  1 user,  load average: 0.00, 0.01, 0.05</span><br><span class="line">Tasks:  83 total,   1 running,  82 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  0.0 us,  0.2 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">KiB Mem :  3880412 total,  1854140 free,   116716 used,  1909556 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.  3456572 avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                            </span><br><span class="line"> 1377 root       0 -20  126352  11548   8920 S   0.3  0.3  13:03.91 AliYunDun                                          </span><br><span class="line">    1 root      20   0   43508   3744   2448 S   0.0  0.1   0:25.31 systemd                                            </span><br><span class="line">    2 root      20   0       0      0      0 S   0.0  0.0   0:00.01 kthreadd                                           </span><br><span class="line">    3 root      20   0       0      0      0 S   0.0  0.0   0:00.11 ksoftirqd/0                                        </span><br><span class="line">    5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H                                       </span><br><span class="line">    6 root      20   0       0      0      0 S   0.0  0.0   0:00.16 kworker/u4:0                                       </span><br><span class="line">    7 root      rt   0       0      0      0 S   0.0  0.0   0:00.04 migration/0                                        </span><br><span class="line">    8 root      20   0       0      0      0 S   0.0  0.0   0:00.00 rcu_bh                                             </span><br><span class="line">    9 root      20   0       0      0      0 S   0.0  0.0   0:39.26 rcu_sched</span><br></pre></td></tr></table></figure><h5 id="查询内存使用情况：free-m"><a href="#查询内存使用情况：free-m" class="headerlink" title="查询内存使用情况：free -m"></a>查询内存使用情况：<code>free -m</code></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# free -m</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           3789         114        1810           0        1864        3375</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure><h5 id="文件系统查询：df"><a href="#文件系统查询：df" class="headerlink" title="文件系统查询：df"></a>文件系统查询：<code>df</code></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1        40G  4.8G   33G  13% /</span><br><span class="line">devtmpfs        1.9G     0  1.9G   0% /dev</span><br><span class="line">tmpfs           1.9G     0  1.9G   0% /dev/shm</span><br><span class="line">tmpfs           1.9G  592K  1.9G   1% /run</span><br><span class="line">tmpfs           1.9G     0  1.9G   0% /sys/fs/cgroup</span><br><span class="line">overlay          40G  4.8G   33G  13% /var/lib/docker/overlay2/20cf413aad752a7988a0667d87698f5dcb6677329bc34184ad0c148950ada89a/merged</span><br><span class="line">shm              64M     0   64M   0% /var/lib/docker/containers/0f9bcc7a9ebb896384bc444019fd4b45aca9a9e63d4b7057c6f1a4a6e1621a35/shm</span><br><span class="line">tmpfs           379M     0  379M   0% /run/user/0</span><br></pre></td></tr></table></figure><h5 id="压缩与解压缩：zip-unzip-gzip-gunzip"><a href="#压缩与解压缩：zip-unzip-gzip-gunzip" class="headerlink" title="压缩与解压缩：zip/unzip     gzip/gunzip"></a>压缩与解压缩：<code>zip/unzip</code>     <code>gzip/gunzip</code></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 压缩需要安装压缩插件</span><br><span class="line">[root@hadoop614 ~]# zip bigdata.sh </span><br><span class="line">-bash: zip: command not found</span><br><span class="line">[root@hadoop614 ~]# yum -y install zip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 压缩为zip文件，源文件不变，重新生成zip压缩文件</span><br><span class="line">[root@hadoop614 ~]# zip -r bigdata.zip bigdata.sh </span><br><span class="line">  adding: bigdata.sh (stored 0%)</span><br><span class="line">[root@hadoop614 ~]# ll bigdata.zip bigdata.sh </span><br><span class="line">-rwxr--r-- 1 root root  36 Jan 26 16:51 bigdata.sh</span><br><span class="line">-rw-r--r-- 1 root root 206 Jan 26 17:37 bigdata.zip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 压缩为gz文件，源文件删除</span><br><span class="line">[root@hadoop614 ~]# gzip bigdata.sh</span><br><span class="line">[root@hadoop614 ~]# ll bigdata.*</span><br><span class="line">-rwxr--r-- 1 root root  68 Jan 26 16:51 bigdata.sh.gz</span><br><span class="line">-rw-r--r-- 1 root root 206 Jan 26 17:37 bigdata.zip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 解压缩需要安装解压缩插件</span><br><span class="line">[root@hadoop614 ~]# unzip bigdata.zip </span><br><span class="line">-bash: unzip: command not found</span><br><span class="line">[root@hadoop614 ~]# yum -y install unzip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 解压zip文件：解压缩后压缩文件和源文件并存</span><br><span class="line">[root@hadoop614 ~]# unzip bigdata.zip </span><br><span class="line">Archive:  bigdata.zip</span><br><span class="line"> extracting: bigdata.sh              </span><br><span class="line">[root@hadoop614 ~]# ll bigdata.*</span><br><span class="line">-rwxr--r-- 1 root root  36 Jan 26 16:51 bigdata.sh</span><br><span class="line">-rwxr--r-- 1 root root  68 Jan 26 16:51 bigdata.sh.gz</span><br><span class="line">-rw-r--r-- 1 root root 206 Jan 26 17:37 bigdata.zip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 解压缩gz文件：解压缩后gz文件消耗四，仅保留解压后的文件</span><br><span class="line">[root@hadoop614 ~]# gunzip bigdata.sh.gz </span><br><span class="line">gzip: bigdata.sh already exists; do you wish to overwrite (y or n)? y</span><br><span class="line">[root@hadoop614 ~]# ll bigdata.*</span><br><span class="line">-rwxr--r-- 1 root root  36 Jan 26 16:51 bigdata.sh</span><br><span class="line">-rw-r--r-- 1 root root 206 Jan 26 17:37 bigdata.zip</span><br></pre></td></tr></table></figure><blockquote><p>小结：<br>zip/unzip 压缩和解压缩后，源文件和压缩文件并存<br>gzip/gunzip 压缩后只有压缩后的文件，解压缩后只有解压缩后的文件</p></blockquote><p>打包压缩多个文件到同一个包内:<code>tar</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">压缩：</span><br><span class="line">[root@hadoop614 ~]# tar -czvf test.tar.gz test*</span><br><span class="line">test1</span><br><span class="line">test2</span><br><span class="line">test3</span><br><span class="line">[root@hadoop614 ~]# ll test*</span><br><span class="line">-rw-r--r-- 1 root root  26 Jan 26 17:48 test1.gz</span><br><span class="line">-rw-r--r-- 1 root root  26 Jan 26 17:48 test2.gz</span><br><span class="line">-rw-r--r-- 1 root root  26 Jan 26 17:48 test3.gz</span><br><span class="line">-rw-r--r-- 1 root root 129 Jan 26 17:49 test.tar.gz</span><br><span class="line">解压缩：</span><br><span class="line">[root@hadoop614 ~]# tar -xzvf test.tar.gz</span><br><span class="line">[root@hadoop614 ~]# ll test*</span><br><span class="line">-rw-r--r-- 1 root root  26 Jan 26 17:48 test1.gz</span><br><span class="line">-rw-r--r-- 1 root root  26 Jan 26 17:48 test2.gz</span><br><span class="line">-rw-r--r-- 1 root root  26 Jan 26 17:48 test3.gz</span><br><span class="line">-rw-r--r-- 1 root root 129 Jan 26 17:49 test.tar.gz</span><br></pre></td></tr></table></figure></p><h5 id="下载URL内容：wget"><a href="#下载URL内容：wget" class="headerlink" title="下载URL内容：wget"></a>下载URL内容：<code>wget</code></h5><blockquote><p>用法：wget URL</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 test]# wget https://mp.csdn.net/postlist</span><br><span class="line">--2019-01-26 17:59:00--  https://mp.csdn.net/postlist</span><br><span class="line">Resolving mp.csdn.net (mp.csdn.net)... 47.95.47.253</span><br><span class="line">Connecting to mp.csdn.net (mp.csdn.net)|47.95.47.253|:443... connected.</span><br><span class="line">HTTP request sent, awaiting response... 307 Temporary Redirect</span><br><span class="line">Location: http://passport.csdn.net/account/login?from=https://mp.csdn.net/postlist [following]</span><br><span class="line">--2019-01-26 17:59:00--  http://passport.csdn.net/account/login?from=https://mp.csdn.net/postlist</span><br><span class="line">Resolving passport.csdn.net (passport.csdn.net)... 101.201.169.146</span><br><span class="line">Connecting to passport.csdn.net (passport.csdn.net)|101.201.169.146|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 301 Moved Permanently</span><br><span class="line">Location: https://passport.csdn.net/account/login?from=https://mp.csdn.net/postlist [following]</span><br><span class="line">--2019-01-26 17:59:00--  https://passport.csdn.net/account/login?from=https://mp.csdn.net/postlist</span><br><span class="line">Connecting to passport.csdn.net (passport.csdn.net)|101.201.169.146|:443... connected.</span><br><span class="line">HTTP request sent, awaiting response... 302 </span><br><span class="line">Location: https://passport.csdn.net/login [following]</span><br><span class="line">--2019-01-26 17:59:00--  https://passport.csdn.net/login</span><br><span class="line">Reusing existing connection to passport.csdn.net:443.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 1121 (1.1K) [text/html]</span><br><span class="line">Saving to: ‘postlist’</span><br><span class="line"></span><br><span class="line">100%[==============================================================================&gt;] 1,121       --.-K/s   in 0s      </span><br><span class="line"></span><br><span class="line">2019-01-26 17:59:00 (211 MB/s) - ‘postlist’ saved [1121/1121]</span><br><span class="line"></span><br><span class="line">[root@hadoop614 test]# cat postlist</span><br><span class="line">&lt;!DOCTYPE html&gt;&lt;html translate=no lang=zh-CN&gt;&lt;head&gt;&lt;meta http-equiv=X-UA-Compatible content=&quot;IE=Edge,chrome=1&quot;&gt;&lt;meta name=referrer content=always&gt;&lt;meta name=renderer content=webkit&gt;&lt;meta name=force-rendering content=webkit&gt;&lt;meta charset=utf-8&gt;&lt;meta name=description content=CSDN登录注册&gt;&lt;meta name=google value=notranslate&gt;&lt;link type=image/x-icon href=https://csdnimg.cn/public/favicon.ico rel=&quot;SHORTCUT ICON&quot;&gt;&lt;title&gt;CSDN-专业IT技术社区&lt;/title&gt;&lt;!--[if lt IE 9]&gt;</span><br><span class="line">       &lt;script&gt;window.location.href=&quot;https://g.csdnimg.cn/browser_upgrade/1.0.2/index.html&quot;;&lt;/script&gt;</span><br><span class="line">    &lt;![endif]--&gt;&lt;link href=https://csdnimg.cn/release/passport_fe/assets/css/login.abab6048cf6e272a47f28638f3aa52ec.css rel=stylesheet&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=app&gt;&lt;/div&gt;&lt;script type=text/javascript src=https://csdnimg.cn/release/passport_fe/assets/js/manifest.603a92bc4219ee818e42.js&gt;&lt;/script&gt;&lt;script type=text/javascript src=https://csdnimg.cn/release/passport_fe/assets/js/vendor.020317dbffa47ab1eb8b.js&gt;&lt;/script&gt;&lt;script type=text/javascript src=https://csdnimg.cn/release/passport_fe/assets/js/login.5853170f9532597487d3.js&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;</span><br></pre></td></tr></table></figure><ul><li><h5 id="调度任务-计划任务：crontab"><a href="#调度任务-计划任务：crontab" class="headerlink" title="调度任务/计划任务：crontab"></a>调度任务/计划任务：<code>crontab</code></h5><blockquote><p>编辑创建任务：crontab -e    # vi编辑模式<br>查询任务内容：crontab -l</p></blockquote></li></ul><p>编辑脚本内容为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# cat bat</span><br><span class="line">#!/bin/bash </span><br><span class="line">for((i=1;i&lt;=6;i++));</span><br><span class="line">do</span><br><span class="line">        date</span><br><span class="line">        sleep 10s</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# crontab -l</span><br><span class="line">* * * * * /root/bat &gt;&gt; /root/bat.log# 书写规则： 分 时 日 月 周 执行的命令  * 表示’每‘</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# tail -F bat.log </span><br><span class="line">Sat Jan 26 18:14:01 CST 2019</span><br><span class="line">Sat Jan 26 18:14:11 CST 2019</span><br><span class="line">Sat Jan 26 18:14:21 CST 2019</span><br></pre></td></tr></table></figure><ul><li><h5 id="后台执行命令-脚本：nohup-xxxx-amp"><a href="#后台执行命令-脚本：nohup-xxxx-amp" class="headerlink" title="后台执行命令/脚本：nohup xxxx &amp;"></a>后台执行命令/脚本：<code>nohup xxxx &amp;</code></h5>前台执行结果：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# ./bat &gt;&gt; bat.log</span><br></pre></td></tr></table></figure></li></ul><p>后台执行结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# nohup ./bat &gt;&gt; bat.log 2&gt;&amp;1 &amp;</span><br><span class="line">[2] 10691</span><br><span class="line">[root@hadoop614 ~]# ps -ef | grep 10691 | grep -v grep</span><br><span class="line">root     10691 10335  0 18:18 pts/0    00:00:00 /bin/bash ./bat</span><br><span class="line">root     10703 10691  0 18:18 pts/0    00:00:00 sleep 10s</span><br><span class="line">[root@hadoop614 ~]# kill -9 10691</span><br><span class="line">[root@hadoop614 ~]# </span><br><span class="line">[2]+  Killed                  nohup ./bat &gt;&gt; bat.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure></p><blockquote><p>详细的输出重定向介绍：<br><a href="https://blog.csdn.net/weixin_42330251/article/details/86531434" target="_blank" rel="noopener">【若泽大数据第一天】基础-Linux基础命令一</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习Linux基础命令第三天&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Linux基础" scheme="http://IToceanxh.github.io/tags/Linux%E5%9F%BA%E7%A1%80/"/>
    
      <category term="运维" scheme="http://IToceanxh.github.io/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>Linux基础命令二</title>
    <link href="http://IToceanxh.github.io/2019/01/23/Linux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E4%BA%8C/"/>
    <id>http://IToceanxh.github.io/2019/01/23/Linux基础命令二/</id>
    <published>2019-01-23T09:45:20.000Z</published>
    <updated>2019-05-01T09:24:34.264Z</updated>
    
    <content type="html"><![CDATA[<p>学习Linux基础命令第二天</p><a id="more"></a><h2 id="Linux基础命令二"><a href="#Linux基础命令二" class="headerlink" title="Linux基础命令二"></a>Linux基础命令二</h2><ul><li>查询历史命令记录：<code>history</code><blockquote><p>历史执行记录保存在 ~/.bash_historty文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# history </span><br><span class="line">    1  echo &gt;.bash_history </span><br><span class="line">    2  history </span><br><span class="line">    3  vi .bash_history </span><br><span class="line">    4  exit</span><br><span class="line">[root@hadoop614 ~]# !3# 执行history命令中第三条指令</span><br></pre></td></tr></table></figure></blockquote></li></ul><ul><li>用户和组：<blockquote><p>创建用户：useradd<br>删除用户：userdel<br>编辑用户：usermod<br>创建组：groupadd<br>删除组：groupdel<br>编辑组：groupmod</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# useradd ruoze</span><br><span class="line">[root@hadoop614 ~]# id ruoze</span><br><span class="line">uid=1001(ruoze) gid=1002(ruoze) groups=1002(ruoze)</span><br></pre></td></tr></table></figure></blockquote></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# groupadd data</span><br><span class="line">[root@hadoop614 ~]# cat /etc/group | grep data</span><br><span class="line">data:x:1003:</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 一个用户可以属于多个组，但必须有切只有一个主组</span><br><span class="line">[root@hadoop614 ~]# usermod -a -G data ruoze</span><br><span class="line">[root@hadoop614 ~]# id ruoze</span><br><span class="line">uid=1001(ruoze) gid=1002(ruoze) groups=1002(ruoze),1003(data)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">## 同一个组可以有多个用户</span><br><span class="line">[root@hadoop614 ~]# usermod -a -G hadoop001 ruoze</span><br><span class="line">[root@hadoop614 ~]# cat /etc/group | grep hadoop001</span><br><span class="line">hadoop001:x:1001:hadoop,ruoze</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">## 用户删除后，主组内无其他用户，则主组自动删除</span><br><span class="line">[root@hadoop614 ~]# userdel ruoze</span><br><span class="line">[root@hadoop614 ~]# id ruoze</span><br><span class="line">id: ruoze: no such user</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# groupdel data</span><br><span class="line">[root@hadoop614 ~]# cat /etc/group | grep data</span><br><span class="line">[root@hadoop614 ~]#</span><br></pre></td></tr></table></figure><ul><li><p>设置密码：<code>passwd</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># passwd 后制定要修改密码的用户，不指定则默认为当前用户</span><br><span class="line">[root@hadoop614 ~]# passwd ruoze</span><br><span class="line">Changing password for user ruoze.</span><br><span class="line">New password: </span><br><span class="line">BAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word</span><br><span class="line">Retype new password: </span><br><span class="line">passwd: all authentication tokens updated successfully.</span><br></pre></td></tr></table></figure></li><li><p>切换用户：<code>su</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# su - ruoze# - 在切换用户时加载用户环境变量并切换到用户家目录</span><br><span class="line">[ruoze@hadoop614 ~]$ pwd</span><br><span class="line">/home/ruoze</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# su ruoze# 不加-，在切换用户时仅加载用户环境变量</span><br><span class="line">[ruoze@hadoop614 root]$ pwd</span><br><span class="line">/root</span><br></pre></td></tr></table></figure><ul><li>普通用户临时获取root权限：<code>sudo</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# vi /etc/sudoers</span><br><span class="line">添加修改一下内容</span><br><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root    ALL=(ALL)       ALL</span><br><span class="line">ruoze   ALL=(root)      NOPASSWD:ALL</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# su - ruoze</span><br><span class="line">Last login: Wed Jan 23 14:34:57 CST 2019 on pts/0</span><br><span class="line">[ruoze@hadoop614 ~]$ ll /root</span><br><span class="line">ls: cannot open directory /root: Permission denied</span><br><span class="line">[ruoze@hadoop614 ~]$ sudo ls -lrt /root</span><br><span class="line">total 4</span><br><span class="line">drwxr-xr-x 2 root root 4096 Jan 20 13:10 3</span><br></pre></td></tr></table></figure><ul><li>筛选/过滤：<code>grep</code></li><li><p>管道/过滤：<code>|</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 将 | 前的执行结果作为后面的执行输入</span><br><span class="line">[root@hadoop614 ~]# cat /etc/passwd | grep ruoze #  在 | 前的执行结果中过滤出包含‘ruoze’的一行数据</span><br><span class="line">ruoze:x:1001:1002::/home/ruoze:/bin/bash</span><br></pre></td></tr></table></figure></li><li><p>退出：<code>exit</code></p></li><li><p>查看进程：<code>ps</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# ps -ef | grep sshd</span><br><span class="line">root      3292     1  0 Jan21 ?        00:00:00 /usr/sbin/sshd -D# ssh程序进程</span><br><span class="line">root      7341  3292  0 14:25 ?        00:00:00 sshd: root@pts/0# root 用户登录进程</span><br><span class="line">root      7492  7343  0 14:44 pts/0    00:00:00 grep --color=auto sshd# grep 过滤进程</span><br><span class="line">#进程所属用户pidppid日期时间进程名</span><br></pre></td></tr></table></figure></li><li><p>杀死进程：<code>kill</code>    </p><blockquote><p><a href="https://blog.csdn.net/weixin_42330251/article/details/86610427" target="_blank" rel="noopener">【Linux基础命令】kill详解</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 前面的先不看，就是输出tail进程的pid，kill -9 进行杀死进行</span><br><span class="line">[root@hadoop614 ~]# ps -ef | grep tail | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9</span><br></pre></td></tr></table></figure></blockquote></li><li><p>查看进程的端口号：<code>netstat</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 不是所有的进程都有端口号</span><br><span class="line">[root@hadoop614 ~]# ps -ef | grep tail | grep -v grep | awk &apos;&#123;print $2&#125;&apos; </span><br><span class="line">7562</span><br><span class="line">[root@hadoop614 ~]# netstat -nlp | grep 7562# tail命令的进程没有端口号</span><br><span class="line">[root@hadoop614 ~]#</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# ps -ef | grep /usr/sbin/sshd | grep -v grep  | awk &apos;&#123;print $2&#125;&apos;</span><br><span class="line">3292</span><br><span class="line">[root@hadoop614 ~]# netstat -nlp | grep 3292# ssh服务可查询到端口</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      3292/sshd </span><br><span class="line"># 协议 提供服务的地址:端口允许访问地址段：端口监听</span><br></pre></td></tr></table></figure><blockquote><p>拒绝错误 Connection refused：<br>1、查看服务器应用进程是否存在: ps -ef | grep xxxx<br>2、查看进程端口是否正确: netstat -nlp | grep $pid<br>3、查看端口允许访问的服务器地址段</p><ul><li>搜索：<code>find</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># find 目录 -name &apos;*名字*&apos;</span><br><span class="line">[root@hadoop614 ~]# find ./ -name ruoz</span><br><span class="line">./3/ruoz</span><br></pre></td></tr></table></figure></li></ul></blockquote><ul><li>安装软件包：<code>yum</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# yum search http# 查看http属于哪一个安装包</span><br><span class="line">[root@hadoop614 ~]# yum install -y  httpd#下载并安装rpm安装包</span><br><span class="line">[root@hadoop614 ~]# service httpd restart# 重新启动httpd服务</span><br><span class="line">Redirecting to /bin/systemctl restart httpd.service</span><br><span class="line">[root@hadoop614 ~]# rpm -qa|grep httpd# 查看httpd版本包名称</span><br><span class="line">httpd-tools-2.4.6-88.el7.centos.x86_64</span><br><span class="line">httpd-2.4.6-88.el7.centos.x86_64</span><br><span class="line">[root@hadoop614 ~]# rpm -e httpd-2.4.6-88.el7.centos.x86_64# 删除/卸载httpd版本程序</span><br><span class="line">[root@hadoop614 ~]# rpm -qa|grep httpd</span><br><span class="line">httpd-tools-2.4.6-88.el7.centos.x86_64</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 删除/卸载使可能出现一下错误</span><br><span class="line">[root@hadoop001 ~]# rpm -e httpd-2.2.15-69.el6.centos.x86_64</span><br><span class="line">error: Failed dependencies:</span><br><span class="line">        httpd &gt;= 2.2.0 is needed by (installed) gnome-user-share-2.28.2-3.el6.x86_64</span><br><span class="line"># 使用--nodeps参数跳过版本关联检测</span><br><span class="line">[root@hadoop001 ~]# rpm -e --nodeps httpd-2.2.15-69.el6.centos.x86_64</span><br><span class="line">warning: /etc/httpd/conf/httpd.conf saved as /etc/httpd/conf/httpd.conf.rpmsave</span><br><span class="line">[root@hadoop001 ~]# rpm -qa|grep httpd</span><br><span class="line">httpd-tools-2.2.15-69.el6.centos.x86_64</span><br></pre></td></tr></table></figure><ul><li><p>查找命令的可执行文件位置：<code>which</code></p><blockquote><p>说明：which命令使从用户的环境变量$PATH中逐层查找</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# which ls</span><br><span class="line">alias ls=&apos;ls --color=auto&apos;</span><br><span class="line">/usr/bin/ls</span><br></pre></td></tr></table></figure></blockquote></li><li><p>查找命令所在的相关路径：<code>whereis</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# whereis ls</span><br><span class="line">ls: /usr/bin/ls /usr/share/man/man1/ls.1.gz</span><br></pre></td></tr></table></figure></li><li><p>帮助与详情：<code>--help 和 man</code></p><blockquote><p>使用方法:<br>ls –help        #命令  –help<br>man ls        # man 命令</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习Linux基础命令第二天&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Linux基础" scheme="http://IToceanxh.github.io/tags/Linux%E5%9F%BA%E7%A1%80/"/>
    
      <category term="运维" scheme="http://IToceanxh.github.io/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>VIM编辑器常用命令详解（基础篇）</title>
    <link href="http://IToceanxh.github.io/2019/01/23/VIM%E7%BC%96%E8%BE%91%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9F%BA%E7%A1%80%E7%AF%87%EF%BC%89/"/>
    <id>http://IToceanxh.github.io/2019/01/23/VIM编辑器常用命令详解（基础篇）/</id>
    <published>2019-01-23T08:54:28.000Z</published>
    <updated>2019-05-01T09:24:12.304Z</updated>
    
    <content type="html"><![CDATA[<p><strong>一、vi/vim是什么</strong><br>Linux世界几乎所有的配置文件都是以纯文本形式存在的，而在所有的Linux发行版系统上都有vi编辑器，因此利用简单的文字编辑软件就能够轻松地修改系统的各种配置了，非常方便。vi就是一种功能强大的文本编辑器，而vim则是高级版的vi，不但可以用不同颜色显示文字内容，还能进行诸如shell脚本、C语言程序编辑等功能，可以作为程序编辑器。<br><a id="more"></a></p><h5 id="二、为什么要学习vi-vim？"><a href="#二、为什么要学习vi-vim？" class="headerlink" title="二、为什么要学习vi/vim？"></a>二、为什么要学习vi/vim？</h5><blockquote><p>   首先所有的Linux发行版系统上都会默认内置vi编辑器，而不一定带有其他文本编辑器，非常通用；<br>其次，很多软件的编辑接口都会默认调用vi；<br>第三，vi具有程序编辑的能力；<br>最后，vi程序简单，编辑速度相当快速。\</p></blockquote><h5 id="三、vi的三种模式及各个模式之间的转换关系"><a href="#三、vi的三种模式及各个模式之间的转换关系" class="headerlink" title="三、vi的三种模式及各个模式之间的转换关系"></a>三、vi的三种模式及各个模式之间的转换关系</h5><p><img src="https://img-blog.csdnimg.cn/20190123163207674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjMzMDI1MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h5 id="四、常用快捷方式"><a href="#四、常用快捷方式" class="headerlink" title="四、常用快捷方式"></a>四、常用快捷方式</h5><p><strong>1.光标移动</strong></p><table><thead><tr><th>按键</th><th>光标动作</th></tr></thead><tbody><tr><td>h j k l ,  ← ↓ ↑ →</td><td>移动一位，h j k l 代表左、下、上、右</td></tr><tr><td>数字 0</td><td>移至本行开头</td></tr><tr><td>^</td><td>移至本行第一个非空字符，匹配开头</td></tr><tr><td>$</td><td>移至本行结尾，可以包含空格</td></tr><tr><td>w</td><td>移至下一单词或标点的开头</td></tr><tr><td>W</td><td>移至下一单词开头，忽略标点</td></tr><tr><td>b</td><td>移至上一单词或标点开头</td></tr><tr><td>B</td><td>移至上一单词开头，忽略标点</td></tr><tr><td>ctrl-f/PgDn</td><td>下翻一页</td></tr><tr><td>ctrl-b/PgUp</td><td>上翻一页</td></tr><tr><td>nG</td><td>移至第n行</td></tr><tr><td>G</td><td>移至光标最后一行</td></tr><tr><td>: n  enter键</td><td>移至第n行</td></tr><tr><td>n+</td><td>向下跳n行</td></tr><tr><td>n-</td><td>向上跳n行</td></tr><tr><td>H</td><td>移至当前屏幕的第一行</td></tr><tr><td>L</td><td>移至当前屏幕的最后一行</td></tr></tbody></table><blockquote><p>注：许多vi的命令前面都可以缀上数字，前缀数字可以控制该命令执行的次数，比如5j可以使得光标向下移动5行。</p></blockquote><p><strong>2.基本编辑</strong></p><table><thead><tr><th>按键</th><th>光标动作 </th></tr></thead><tbody><tr><td>a</td><td>在当前字符后插入文本</td></tr><tr><td>A</td><td>在行尾插入文本</td></tr><tr><td>i</td><td>在当前字符前插入文本</td></tr><tr><td>I</td><td>在行首插入文本</td></tr><tr><td>o</td><td>当前行下方插入新行并进入插入模式</td></tr><tr><td>O</td><td>当前行上方插入新行并进入插入模式</td></tr></tbody></table><p><strong>3. 删除和撤销</strong></p><table><thead><tr><th>按键</th><th>光标动作 </th></tr></thead><tbody><tr><td>x</td><td>删除当前字符（剪切）</td></tr><tr><td>nx</td><td>向后删除当前行在内的n个字符（剪切）</td></tr><tr><td>dd</td><td>删除（剪切）当前行</td></tr><tr><td>ndd</td><td>向下删除当前行在内的n行（剪切）</td></tr><tr><td>dW</td><td>删除当前字符到下一单词的起始处（删除整个单词）</td></tr><tr><td>d$</td><td>删除当前字符到当前行的末尾（剪切）</td></tr><tr><td>d0</td><td>删除当前字符到当前行的起始处（剪切）</td></tr><tr><td>d^</td><td>删除当前字符当当前行下一个非空字符（剪切）</td></tr><tr><td>dG</td><td>删除当前行到文件末尾（剪切）</td></tr><tr><td>d20G</td><td>删除当前行到文件第20行（d与定位符结合使用，x不行）（剪切）</td></tr><tr><td>u</td><td>撤销上一次操作</td></tr><tr><td>nu</td><td>撤销n次操作</td></tr><tr><td>U</td><td>撤销对当前行的所有操作</td></tr></tbody></table><p><strong>4.剪切、复制、粘贴</strong></p><blockquote><p>注意：x和d键实质上是剪切键，两者功能略有差异。y是复制键，p是粘贴键。</p></blockquote><table><thead><tr><th>键</th><th>光标动作</th></tr></thead><tbody><tr><td>yy</td><td>复制当前行</td></tr><tr><td>5yy</td><td>向下复制当前行在内的5行</td></tr><tr><td>yw</td><td>当前字符到下一单词的起始处</td></tr><tr><td>y$</td><td>当前字符到当前行的末尾</td></tr><tr><td>y^</td><td>当前字符到当前行下一个非空字符</td></tr><tr><td>y0</td><td>当前字符到当前行的行首</td></tr><tr><td>yG</td><td>当前行到文件末尾</td></tr><tr><td>y20G</td><td>当前行到文件第20行（用法和d完全一样，d实际就是剪切）</td></tr><tr><td>p</td><td>粘贴到当前字符（行）后面（下方）</td></tr><tr><td>P</td><td>粘贴到当前字符（行）前面（上方）</td></tr><tr><td>J</td><td>合并当前行和下一行为一行</td></tr><tr><td>R</td><td>替换模式，和windows下的insert模式差不多</td></tr><tr><td>v</td><td>进入选择模式（VISUAL），可以利用方向键选择字符，然后x、d剪切或者y复制</td></tr></tbody></table><p><strong>5.查找和替换</strong></p><table><thead><tr><th>键</th><th>光标动作</th></tr></thead><tbody><tr><td>fx（当前行内查找）</td><td>光标移至本行下一处出现字符x的位置，输入分号重复上一次搜索</td></tr><tr><td>/（搜索单词或短语）</td><td>使用/命令后，屏幕底端会出现/，接下来输入要搜索的单词或短语，enter结束</td></tr><tr><td>: s/old/new</td><td>用new替换行中首次出现的old（:分号用于启动一条ex命令）</td></tr><tr><td>: s/old/new/g</td><td>用new替换行中所有的old</td></tr><tr><td>:n,m s/old/new/g</td><td>用new替换从n到m行里所有的old</td></tr><tr><td>:%s/old/new/g</td><td>用new替换当前文件里所有的old</td></tr><tr><td>:set nu</td><td>显示行号</td></tr><tr><td>:set nonu</td><td>取消显示行号</td></tr></tbody></table><p><strong>6.保存工作</strong></p><table><thead><tr><th>键</th><th>光标动作</th></tr></thead><tbody><tr><td>:w</td><td>仅保存文件</td></tr><tr><td>:q</td><td>退出文件，没有修改</td></tr><tr><td>:q!</td><td>放弃修改并退出文件</td></tr><tr><td>:wq</td><td>保存并退出文件</td></tr><tr><td>:w file1</td><td>文件另存为file1</td></tr><tr><td>ZZ</td><td>保存并退出文件</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;一、vi/vim是什么&lt;/strong&gt;&lt;br&gt;Linux世界几乎所有的配置文件都是以纯文本形式存在的，而在所有的Linux发行版系统上都有vi编辑器，因此利用简单的文字编辑软件就能够轻松地修改系统的各种配置了，非常方便。vi就是一种功能强大的文本编辑器，而vim则是高级版的vi，不但可以用不同颜色显示文字内容，还能进行诸如shell脚本、C语言程序编辑等功能，可以作为程序编辑器。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://IToceanxh.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Linux基础" scheme="http://IToceanxh.github.io/tags/Linux%E5%9F%BA%E7%A1%80/"/>
    
      <category term="运维" scheme="http://IToceanxh.github.io/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>kill详解</title>
    <link href="http://IToceanxh.github.io/2019/01/23/kill%E8%AF%A6%E8%A7%A3/"/>
    <id>http://IToceanxh.github.io/2019/01/23/kill详解/</id>
    <published>2019-01-23T07:58:36.000Z</published>
    <updated>2019-05-01T09:25:20.443Z</updated>
    
    <content type="html"><![CDATA[<p>简单描述 Linux 系统的 <code>kill</code> 命令</p><a id="more"></a><h4 id="命令格式"><a href="#命令格式" class="headerlink" title="命令格式"></a>命令格式</h4><blockquote><p>kill [参数] [进程号]</p></blockquote><h4 id="命令功能"><a href="#命令功能" class="headerlink" title="命令功能"></a>命令功能</h4><blockquote><p>发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用“-KILL” 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。</p></blockquote><h4 id="命令参数"><a href="#命令参数" class="headerlink" title="命令参数"></a>命令参数</h4><h5 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h5><table><thead><tr><th>参数</th><th>参数介绍</th></tr></thead><tbody><tr><td>-l</td><td>信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称</td></tr><tr><td>-a</td><td>当处理当前进程时，不限制命令名和进程号的对应关系</td></tr><tr><td>-p</td><td>指定kill 命令只打印相关进程的进程号，而不发送任何信号</td></tr><tr><td>-s</td><td>指定发送信号</td></tr><tr><td>-u</td><td>指定用户 </td></tr></tbody></table><h5 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h5><ol><li>kill命令可以带信号号码选项，也可以不带。如果没有信号号码，kill命令就会发出终止信号(15)，这个信号可以被进程捕获，使得进程在退出之前可以清理并释放资源。也可以用kill向进程发送特定的信号。例如：kill -2 123<br>它的效果等同于在前台运行PID为123的进程时按下Ctrl+C键。但是，普通用户只能使用不带signal参数的kill命令或最多使用-9信号。</li><li>kill可以带有进程ID号作为参数。当用kill向这些进程发送信号时，必须是这些进程的主人。如果试图撤销一个没有权限撤销的进程或撤销一个不存在的进程，就会得到一个错误信息。</li><li>可以向多个进程发信号或终止它们。</li><li>当kill成功地发送了信号后，shell会在屏幕上显示出进程的终止信息。有时这个信息不会马上显示，只有当按下Enter键使shell的命令提示符再次出现时，才会显示出来。</li><li>应注意，信号使进程强行终止，这常会带来一些副作用，如数据丢失或者终端无法恢复到正常状态。发送信号时必须小心，只有在万不得已时，才用kill信号(9)，因为进程不能首先捕获它。要撤销所有的后台作业，可以输入kill 0。因为有些在后台运行的命令会启动多个进程，跟踪并找到所有要杀掉的进程的PID是件很麻烦的事。这时，使用kill 0来终止所有由当前shell启动的进程，是个有效的方法。</li></ol><h4 id="使用实例"><a href="#使用实例" class="headerlink" title="使用实例"></a>使用实例</h4><p>实例1：列出所有信号名称<br>命令：<code>kill -l</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop614 ~]# kill -l</span><br><span class="line"> 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP</span><br><span class="line"> 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL10) SIGUSR1</span><br><span class="line">11) SIGSEGV12) SIGUSR213) SIGPIPE14) SIGALRM15) SIGTERM</span><br><span class="line">16) SIGSTKFLT17) SIGCHLD18) SIGCONT19) SIGSTOP20) SIGTSTP</span><br><span class="line">21) SIGTTIN22) SIGTTOU23) SIGURG24) SIGXCPU25) SIGXFSZ</span><br><span class="line">26) SIGVTALRM27) SIGPROF28) SIGWINCH29) SIGIO30) SIGPWR</span><br><span class="line">31) SIGSYS34) SIGRTMIN35) SIGRTMIN+136) SIGRTMIN+237) SIGRTMIN+3</span><br><span class="line">38) SIGRTMIN+439) SIGRTMIN+540) SIGRTMIN+641) SIGRTMIN+742) SIGRTMIN+8</span><br><span class="line">43) SIGRTMIN+944) SIGRTMIN+1045) SIGRTMIN+1146) SIGRTMIN+1247) SIGRTMIN+13</span><br><span class="line">48) SIGRTMIN+1449) SIGRTMIN+1550) SIGRTMAX-1451) SIGRTMAX-1352) SIGRTMAX-12</span><br><span class="line">53) SIGRTMAX-1154) SIGRTMAX-1055) SIGRTMAX-956) SIGRTMAX-857) SIGRTMAX-7</span><br><span class="line">58) SIGRTMAX-659) SIGRTMAX-560) SIGRTMAX-461) SIGRTMAX-362) SIGRTMAX-2</span><br><span class="line">63) SIGRTMAX-164) SIGRTMAX</span><br></pre></td></tr></table></figure></p><p><span>==本人平时只是用kill -9的参数，百度看到飘飘雪的内容很详细，内容均来自于飘飘雪博客==</span><br><span><a href="https://www.cnblogs.com/wangcp-2014/p/5146343.html" target="_blank" rel="noopener">博客园–飘飘雪(linux kill命令详解)</a></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单描述 Linux 系统的 &lt;code&gt;kill&lt;/code&gt; 命令&lt;/p&gt;
    
    </summary>
    
      <category term="运维" scheme="http://IToceanxh.github.io/categories/%E8%BF%90%E7%BB%B4/"/>
    
    
      <category term="若泽数据" scheme="http://IToceanxh.github.io/tags/%E8%8B%A5%E6%B3%BD%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Linux基础" scheme="http://IToceanxh.github.io/tags/Linux%E5%9F%BA%E7%A1%80/"/>
    
      <category term="运维" scheme="http://IToceanxh.github.io/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
</feed>
